
# üìñ Table of Contents ‚Äî Understanding Camera Models in Detail

---

## Part I: Foundations of Camera Models

1. **Introduction to Camera Models**

   * Why do we need camera models?
   * Applications in computer vision, robotics, AR/VR
   * Historical note: pinhole to modern digital cameras

2. **Geometry of Image Formation**

   * Light, rays, and projection
   * The concept of the optical center
   * The image plane vs. the sensor plane

3. **Homogeneous Coordinates**

   * Why we use homogeneous coordinates
   * Representing points, lines, and transformations
   * Projective geometry basics

---

## Part II: Classical Camera Models

4. **The Pinhole Camera Model**

   * Basic setup and assumptions
   * Perspective projection equations
   * Image inversion and virtual image plane
   * Aperture size trade-offs

5. **Lens-Based Models**

   * The thin lens equation
   * Depth of field and focus
   * Lens distortions (barrel, pincushion, tangential)

6. **Alternative Simplified Models**

   * Weak perspective model
   * Orthographic projection
   * Scaled orthographic (paraperspective)
   * When approximations are useful

---

## Part III: Mathematical Formulation of Camera Projection

7. **Extrinsic Parameters (World ‚Üí Camera)**

   * Camera coordinate system
   * Rotation matrix \$R\$
   * Translation vector \$t\$
   * Rigid body transformation

8. **Intrinsic Parameters (Camera ‚Üí Pixels)**

   * Effective focal lengths \$f\_x, f\_y\$
   * Principal point \$(c\_x, c\_y)\$
   * Skew and pixel aspect ratio
   * The calibration matrix \$K\$

9. **The Full Camera Matrix**

   * Derivation: \$P = K \[R|t]\$
   * Properties and degrees of freedom
   * Geometric interpretation

10. **Projection in Homogeneous Coordinates**

    * Matrix form of projection
    * Perspective divide
    * From 3D points to 2D pixels

---

## Part IV: Practical Considerations in Real Cameras

11. **Lens Distortion Models**

    * Radial distortion
    * Tangential distortion
    * Modeling and correction

12. **Sensor Characteristics**

    * Discretization into pixels
    * Noise and quantization
    * Rolling shutter effects

13. **Multi-Camera Systems**

    * Stereo geometry and epipolar constraint
    * Structure from motion (SfM)
    * Camera networks and panoramic cameras

---

## Part V: Camera Calibration

14. **Why Calibration Matters**

    * Estimating intrinsics and extrinsics
    * Applications in measurement and 3D vision

15. **Linear Camera Calibration**

    * Direct Linear Transform (DLT)
    * Requirements and degenerate cases

16. **Nonlinear Optimization (Bundle Adjustment)**

    * Reprojection error minimization
    * Distortion parameter estimation

17. **Practical Calibration Pipelines**

    * Checkerboard calibration
    * OpenCV implementation
    * Evaluating calibration accuracy

---

## Part VI: Advanced Topics

18. **Special Camera Models**

    * Fish-eye cameras
    * Catadioptric (mirror + lens) systems
    * Omnidirectional cameras

19. **Camera Model Extensions**

    * Projective ambiguity and self-calibration
    * Multi-view geometry basics
    * Absolute vs. relative camera pose

20. **Modern Applications of Camera Models**

    * Augmented reality & pose estimation
    * SLAM and visual odometry
    * Neural rendering (NeRFs, differentiable cameras)

---

---

‚ö° This TOC builds from **basic geometry ‚Üí classical models ‚Üí full camera matrix ‚Üí calibration ‚Üí advanced models and applications**.





# üì∑ Part I: Foundations of Camera Models

---

## 1. **Introduction to Camera Models**

### üîç Why Do We Need Camera Models?

In computer vision, the camera is the interface between the **3D world** and a **2D digital image**. A **camera model** is a mathematical abstraction that describes this transformation.

Mathematically, a 3D point

$$
X = (X, Y, Z, 1)^T
$$

is projected to a 2D pixel

$$
x = (u, v, 1)^T
$$

through a **camera projection function**:

$$
x \sim P X
$$

where $P \in \mathbb{R}^{3 \times 4}$ is the **camera matrix**.

Without a model, tasks like 3D reconstruction, augmented reality overlays, or robot navigation would be impossible ‚Äî because we couldn‚Äôt reason how the 2D images relate back to the 3D world.

---

### ‚öôÔ∏è Applications in Vision and Robotics

* **Computer Vision**: 3D reconstruction, depth estimation, object recognition.
* **Robotics**: visual SLAM (Simultaneous Localization and Mapping), robot navigation.
* **AR/VR**: overlaying graphics aligned with the physical world.
* **Industrial/Medical**: photogrammetry, medical imaging, surgical navigation.

---

### üï∞ Historical Note: From Pinhole to Digital Cameras

* **Pinhole Camera (5th century BCE)**: used by Mozi (China) and Aristotle (Greece) ‚Äî a dark chamber with a tiny hole projects an inverted image.
* **Renaissance Perspective (15th century)**: Alberti and Brunelleschi formalized projection geometry for art.
* **19th‚Äì20th century**: photographic film and glass lenses dominate.
* **Modern Era**: CCD/CMOS sensors replace film, but the geometry (pinhole model + distortions) still forms the mathematical backbone.

---

## 2. **Geometry of Image Formation**

### üåû Light, Rays, and Projection

* Each **3D point** emits or reflects light rays in all directions.
* The **camera aperture (pinhole)** restricts rays so that exactly **one ray per 3D point** reaches the image plane.
* This ensures a **unique mapping**: one world point ‚Üí one image point.

**Mathematical Setup:**
Let the **camera coordinate system** have origin at the optical center $O$, with the $Z$-axis pointing forward.
A world point in camera coordinates is

$$
X_c = (X_c, Y_c, Z_c)^T
$$

The ray from $O$ through $X_c$ intersects the **image plane** at distance $f$ (focal length) along the $Z$-axis.

By similar triangles (see diagram placeholder below):

$$
x_i = \frac{f X_c}{Z_c}, \quad y_i = \frac{f Y_c}{Z_c}
$$

This is the **perspective projection equation**.

üëâ **Figure Placeholder**: Show a 3D point, pinhole at origin, rays intersecting the image plane at $f$.

---

### üéØ The Concept of the Optical Center

* The **optical center** (a.k.a. camera center or projection center) is the point where all rays converge.
* In the pinhole model, it‚Äôs a single idealized point.
* In real cameras, it corresponds approximately to the center of the entrance pupil of the lens system.

All rays pass through this center ‚Üí projection is a **central projection**.

---

### üñº The Image Plane vs. the Sensor Plane

* In pure geometry, the **image plane** is placed **behind the pinhole** at distance $f$.
* This produces an **inverted image**.
* Alternatively, we can place a **virtual image plane** in front of the pinhole (same math up to a scale inversion).

In real cameras:

* The **sensor plane** (CCD/CMOS) is a discrete pixel array.
* The mapping from continuous $(x_i, y_i)$ (in mm) to pixels $(u,v)$ is handled by the **intrinsic calibration matrix** $K$.

---

## 3. **Homogeneous Coordinates**

### ‚ö° Why Do We Use Homogeneous Coordinates?

Perspective projection involves **division by depth** ($Z_c$):

$$
x_i = \frac{f X_c}{Z_c}, \quad y_i = \frac{f Y_c}{Z_c}
$$

This is nonlinear. But using **homogeneous coordinates**, we can write projection as a **linear matrix multiplication**.

---

### ‚úç Representing Points

* A 2D point $(u,v)$ is represented as $(u,v,1)^T$.
* A 3D point $(X,Y,Z)$ becomes $(X,Y,Z,1)^T$.
* More generally:

$$
(x,y) \equiv (kx, ky, k) \quad \forall k \neq 0
$$

That is, homogeneous coordinates are defined **up to scale**.

---

### üìê Representing Lines and Transformations

* A 2D line $ax + by + c = 0$ is represented as the vector $(a,b,c)^T$.
* A point $x = (u,v,1)^T$ lies on line $l = (a,b,c)^T$ if:

$$
l^T x = 0
$$

* Geometric transformations (translation, rotation, projection) can be written as **matrices** in homogeneous coordinates.

Example: A 2D translation by $(t_x, t_y)$:

$$
T =
\begin{bmatrix}
1 & 0 & t_x \\
0 & 1 & t_y \\
0 & 0 & 1
\end{bmatrix}
$$

$$
x' = T x
$$

---

### üåê Projective Geometry Basics

Homogeneous coordinates extend Euclidean geometry into **projective geometry**:

* **Points at infinity** (parallel lines meeting at vanishing points) are naturally represented.
* A perspective projection is just a **projective transformation** in homogeneous space.
* Camera projection matrix $P$ is a $3\times4$ projective transform.

üëâ This framework is what allows us to write:

$$
\tilde{x} \sim P \tilde{X}
$$

for any 3D point $\tilde{X}$.

---




---

# üì∑ Part II: Classical Camera Models

---

## 4. **The Pinhole Camera Model**

### üìå Basic Setup and Assumptions

The **pinhole camera** is the most fundamental camera model. It consists of:

* A completely dark box.
* A **tiny aperture (pinhole)** on one side.
* An **image plane** (film/sensor) opposite the hole.

**Assumptions:**

1. The aperture is **infinitesimally small**, allowing exactly one light ray from each 3D point to pass through.
2. No lens, so no refraction effects.
3. Geometry is purely **perspective projection**.

üëâ **Figure Placeholder:** Rays from a 3D point passing through the pinhole and forming an image on the plane.

---

### üìê Perspective Projection Equations

Let the **camera coordinate system** be centered at the pinhole $O$:

* Optical axis = $Z$-axis.
* Image plane at distance $f$ from the pinhole.

For a 3D point in camera coordinates:

$$
X_c = (X_c, Y_c, Z_c)^T
$$

Its projection onto the image plane is found via **similar triangles**:

$$
x_i = \frac{f X_c}{Z_c}, \quad y_i = \frac{f Y_c}{Z_c}
$$

where $(x_i, y_i)$ are in **metric units (mm)** on the image plane.

---

### üîÑ Image Inversion and Virtual Image Plane

* In the **real physical setup**, the image plane is **behind the pinhole**.
* This results in an **inverted image** (upside-down).

To simplify, we often place a **virtual image plane in front of the pinhole**:

* Same perspective geometry, but no inversion.
* Both representations are equivalent up to a reflection.

üëâ **Figure Placeholder:** Real image plane vs. virtual image plane.

---

### üîç Aperture Size Trade-offs

In practice, the aperture cannot be infinitesimal:

| Aperture Size | Brightness   | Sharpness                         |
| ------------- | ------------ | --------------------------------- |
| **Small**     | Dim image    | Sharp (few rays per point)        |
| **Large**     | Bright image | Blurred (multiple rays per point) |

This trade-off motivates **lenses** ‚Üí to admit more light while maintaining focus.

---

## 5. **Lens-Based Models**

### üîé The Thin Lens Equation

Replacing the pinhole with a **convex lens** allows multiple rays from the same point to converge on the image plane.

The **thin lens equation**:

$$
\frac{1}{f} = \frac{1}{z_o} + \frac{1}{z_i}
$$

where:

* $f$ = focal length of lens
* $z_o$ = object distance (from lens to object point)
* $z_i$ = image distance (from lens to image plane)

If the sensor is placed at $z_i$, the object point is in focus.

üëâ **Figure Placeholder:** Rays from near and far objects converging differently depending on sensor placement.

---

### üéØ Depth of Field and Focus

* Only points at a specific distance are in **perfect focus**.
* Points closer/further blur into **circles of confusion**.
* The **range of distances** where the blur is acceptably small = **depth of field (DoF)**.

DoF depends on:

* Aperture size (smaller ‚Üí larger DoF, sharper background).
* Focal length.
* Sensor size.

---

### üìâ Lens Distortions

Real lenses deviate from the thin-lens ideal. Common distortions:

1. **Radial distortion** (symmetric around center):

   * **Barrel**: straight lines bulge outward.
   * **Pincushion**: straight lines pinch inward.

   Approximate model:

   $$
   x_d = x (1 + k_1 r^2 + k_2 r^4 + \dots), \quad 
   y_d = y (1 + k_1 r^2 + k_2 r^4 + \dots)
   $$

   where $r = \sqrt{x^2 + y^2}$.

2. **Tangential distortion** (due to lens misalignment):

   $$
   x_d = x + [2 p_1 x y + p_2 (r^2 + 2x^2)],  
   $$

   $$
   y_d = y + [p_1 (r^2 + 2y^2) + 2p_2 xy]  
   $$

üëâ **Figure Placeholder:** Barrel vs. pincushion distortion grids.

---

## 6. **Alternative Simplified Models**

In many applications, exact perspective projection is unnecessary ‚Äî simpler models suffice.

---

### ü™û Weak Perspective Model

Assumption: All scene points are at approximately the **same depth** $z_0$.

Approximation:

$$
x' = \frac{f}{z_0} X, \quad y' = \frac{f}{z_0} Y
$$

* Equivalent to a uniform **scaling + orthographic projection**.
* Good for **small, distant objects**.

---

### üìê Orthographic Projection

Assumption: Rays are **parallel** to the optical axis (camera at infinity).

Projection:

$$
x' = X, \quad y' = Y
$$

* Depth $Z$ is ignored completely.
* Preserves parallelism (no vanishing points).
* Used in **engineering drawings, CAD**.

---

### ‚öñÔ∏è Scaled Orthographic (Paraperspective) Projection

Hybrid between weak perspective and true perspective:

$$
x' = \frac{f}{Z_0} X, \quad y' = \frac{f}{Z_0} Y
$$

where $Z_0$ = average depth of the object.

* Each object is scaled by a constant depending on its depth.
* More accurate than orthographic, simpler than full perspective.
* Useful in **human pose estimation** and **face alignment**.

---

### ü§î When Approximations Are Useful

* **Orthographic**: good for small objects, far from camera, minimal depth variation.
* **Weak Perspective / Paraperspective**: balance accuracy and simplicity when perspective effects are mild.
* **Perspective (pinhole)**: needed for close-up objects, wide depth range, or precise geometry.

---

## ‚úÖ Wrap-Up of Part II

* **Pinhole camera model**: foundation, perspective projection, aperture trade-offs.
* **Lens models**: thin lens equation, depth of field, distortions.
* **Simplified models**: weak perspective, orthographic, scaled orthographic ‚Äî practical when perspective effects are negligible.

This builds the foundation for **Part III: Mathematical Formulation of Camera Projection**, where we‚Äôll rigorously derive the **camera matrix $P = K [R|t]$**.

---







---

# üìê Part III: Mathematical Formulation of Camera Projection

---

## 7. **Extrinsic Parameters (World ‚Üí Camera)**

### üìç Camera Coordinate System

* We define a **camera coordinate system (CCS)** with:

  * Origin at the **optical center** (pinhole).
  * $Z$-axis along the **optical axis**, pointing forward.
  * $X$-axis to the right, $Y$-axis down (convention used in computer vision).

üëâ Every 3D point must first be expressed in this **camera-centric frame** before projection.

---

### üîÑ Rotation Matrix $R$

Suppose a 3D point in **world coordinates** is:

$$
X_W = (X, Y, Z)^T
$$

The camera may be oriented differently. A **rotation matrix** $R \in SO(3)$:

$$
X' = R X_W
$$

where:

* $R^T R = I$ (orthogonal)
* $\det(R) = +1$

üëâ $R$ changes **basis** from world axes to camera axes.

---

### ‚ûï Translation Vector $t$

If the camera is not at the world origin, we must **translate**:

$$
X_C = R X_W + t
$$

where:

* $t \in \mathbb{R}^3$ is the **camera position** expressed in the camera coordinate frame.

Interpretation: $R$ rotates the world, then $t$ shifts it relative to the camera.

---

### üß© Rigid Body Transformation

In **homogeneous coordinates**:

$$
\begin{bmatrix}
X_C \\ 1
\end{bmatrix}
=
\underbrace{
\begin{bmatrix}
R & t \\
0 & 1
\end{bmatrix}}_{\text{Extrinsic matrix } [R|t]}
\begin{bmatrix}
X_W \\ 1
\end{bmatrix}
$$

This is a **rigid body transformation** (rotation + translation), preserving distances and angles.

---

## 8. **Intrinsic Parameters (Camera ‚Üí Pixels)**

After perspective projection, points are expressed in **camera metric units (mm)**. To map to **pixels**, we need **intrinsic parameters**.

---

### üéØ Effective Focal Lengths $f_x, f_y$

Real sensors have finite pixel density:

* $m_x$: pixels per mm in the horizontal direction.
* $m_y$: pixels per mm in the vertical direction.

Effective focal lengths (in pixels):

$$
f_x = m_x f, \quad f_y = m_y f
$$

---

### üìç Principal Point $(c_x, c_y)$

The **principal point** is where the optical axis intersects the sensor.

* Usually near the image center.
* May be offset due to imperfect sensor alignment.

---

### üîÄ Skew and Pixel Aspect Ratio

* In most cameras, image axes are **orthogonal** ‚Üí skew = 0.
* In some cases (non-square pixels, sensor misalignment):

  * Skew angle $\theta$,
  * Skew parameter: $s = f_x \cot\theta$.

---

### üèó Calibration Matrix $K$

All intrinsics combine into the **upper-triangular calibration matrix**:

$$
K =
\begin{bmatrix}
f_x & s & c_x \\
0 & f_y & c_y \\
0 & 0 & 1
\end{bmatrix}
$$

Thus, given normalized image-plane coordinates $(x_i, y_i, 1)^T$, pixel coordinates are:

$$
x = K \, x_i
$$

---

## 9. **The Full Camera Matrix**

Now we combine **extrinsics** ($R, t$) and **intrinsics** ($K$).

---

### üßÆ Derivation: $P = K [R|t]$

Starting with a homogeneous world point:

$$
\tilde{X}_W = (X, Y, Z, 1)^T
$$

1. **World ‚Üí Camera**:

$$
X_C = [R|t] \tilde{X}_W
$$

2. **Camera ‚Üí Normalized Image Plane** (perspective projection):

$$
x_{img} = \left(\tfrac{X_C}{Z_C}, \tfrac{Y_C}{Z_C}, 1 \right)^T
$$

3. **Image Plane ‚Üí Pixels**:

$$
x = K x_{img}
$$

Combining:

$$
x \sim K [R|t] \tilde{X}_W
$$

Define the **camera projection matrix**:

$$
P = K [R|t] \quad \in \mathbb{R}^{3 \times 4}
$$

Thus:

$$
\boxed{ \; x \sim P \, \tilde{X}_W \;}
$$

---

### üìä Properties and Degrees of Freedom

* $K$: 5 DOF (focal lengths, skew, principal point).
* $[R|t]$: 6 DOF (3 rotation, 3 translation).
* Total = **11 DOF** (since scale of $P$ is arbitrary).

---

### üåê Geometric Interpretation

* Each 2D pixel corresponds to a **ray in 3D**: all points on

$$
X_W(\lambda) = C + \lambda d
$$

map to the same pixel.

* $C$ = camera center,
* $d$ = direction vector through pixel.
* Projection collapses depth ‚Üí **non-invertible**.

---

## 10. **Projection in Homogeneous Coordinates**

### üßæ Matrix Form of Projection

For any 3D world point:

$$
\tilde{x} \sim P \tilde{X}_W
$$

$$
\tilde{x} =
\begin{bmatrix}
u \\ v \\ w
\end{bmatrix}
, \quad
\tilde{X}_W =
\begin{bmatrix}
X \\ Y \\ Z \\ 1
\end{bmatrix}
$$

---

### ‚úÇÔ∏è Perspective Divide

To get pixel coordinates:

$$
(u', v') = \left( \frac{u}{w}, \frac{v}{w} \right)
$$

This final division is what makes the mapping **projective** (nonlinear).

---

### üì∏ From 3D Points to 2D Pixels

Full mapping:

1. World point $(X, Y, Z)$
2. Apply rigid transform ($[R|t]$) ‚Üí camera frame
3. Apply perspective divide ($/Z_c$)
4. Apply intrinsics ($K$) ‚Üí pixel coordinates $(u, v)$

---

## ‚úÖ Wrap-Up of Part III

* **Extrinsics**: camera pose, $[R|t]$.
* **Intrinsics**: sensor geometry, $K$.
* **Full camera matrix**: $P = K [R|t]$, 11 DOF.
* **Homogeneous projection**:

$$
\tilde{x} \sim P \tilde{X}_W, \quad (u,v) = \left( \tfrac{u}{w}, \tfrac{v}{w} \right)
$$

This provides the **mathematical backbone** for calibration, reconstruction, and multi-view geometry.

---







---

# üì∑ Part IV: Practical Considerations in Real Cameras

---

## 11. **Lens Distortion Models**

In an ideal pinhole camera, straight 3D lines project to straight 2D lines.
But real lenses bend rays differently across the field of view, introducing **distortion**.
This is modeled and corrected during calibration.

---

### üîµ Radial Distortion

* Caused by lens curvature.
* Symmetric around the **principal point**.
* Effect: farther from center ‚Üí greater distortion.

Two main types:

1. **Barrel distortion** ‚Üí lines bulge outward.
2. **Pincushion distortion** ‚Üí lines pinch inward.

**Mathematical model:**
Let normalized coordinates be $(x, y)$, with radius:

$$
r^2 = x^2 + y^2
$$

Distorted coordinates:

$$
x_d = x (1 + k_1 r^2 + k_2 r^4 + k_3 r^6),  
$$

$$
y_d = y (1 + k_1 r^2 + k_2 r^4 + k_3 r^6)
$$

where $k_1, k_2, k_3$ = radial distortion coefficients.

üëâ Higher-order terms used for wide-angle / fisheye lenses.

---

### üî∂ Tangential Distortion

* Caused by misaligned lens elements.
* Effect: rays shift tangentially, introducing asymmetry.

**Model:**

$$
x_d = x + [2p_1 xy + p_2(r^2 + 2x^2)]  
$$

$$
y_d = y + [p_1(r^2 + 2y^2) + 2p_2 xy]  
$$

where $p_1, p_2$ = tangential distortion coefficients.

---

### üõ† Modeling and Correction

* Total distorted coordinates:

$$
(u_d, v_d) = f(K, k_1, k_2, k_3, p_1, p_2, u, v)
$$

* During **camera calibration** (e.g., OpenCV‚Äôs `calibrateCamera`), both **intrinsics** $K$ and **distortion coefficients** are estimated simultaneously.
* Correction = compute inverse mapping (often iterative).

üëâ **Figure placeholder**: checkerboard before vs. after distortion correction.

---

## 12. **Sensor Characteristics**

Even after correcting for distortion, the **image sensor** introduces its own limitations.

---

### üü¶ Discretization into Pixels

* Image plane = continuous 2D coordinates $(x_i, y_i)$.
* Sensor samples them into a **finite grid**:

$$
u = \lfloor m_x x_i + c_x \rfloor, \quad v = \lfloor m_y y_i + c_y \rfloor
$$

where $m_x, m_y$ = pixel densities.

üëâ Consequence: **aliasing** (high-frequency patterns look wrong ‚Äî e.g. moir√©).

---

### üîä Noise and Quantization

Sensors are noisy, introducing random errors:

* **Photon shot noise** (statistical variation in photons).
* **Thermal noise** (sensor electronics).
* **Readout noise** (amplification errors).

Digital values are quantized:

* With $b$-bit sensor, intensities are integers in $[0, 2^b - 1]$.
* Example: 8-bit ‚Üí range 0‚Äì255.

---

### üé• Rolling Shutter Effects

Most CMOS sensors use a **rolling shutter**:

* Rows are exposed sequentially (not all at once).
* Fast motion ‚Üí geometric distortions.

Example:

* A vertical propeller looks bent (top rows captured earlier than bottom rows).

**Mathematical model:**
If row $i$ is exposed at time $t_i = t_0 + i \Delta t$, then projection becomes **time-dependent**:

$$
x(t), y(t) \sim P(t) X_W
$$

üëâ Correction requires modeling camera motion across the exposure.

---

## 13. **Multi-Camera Systems**

Many tasks need **depth recovery** or a wider field of view than a single pinhole camera can provide.

---

### üîÄ Stereo Geometry and Epipolar Constraint

Two cameras observing the same scene define **epipolar geometry**.

* $C_L, C_R$: camera centers.
* $X$: 3D world point.
* $x_L, x_R$: corresponding projections.

Constraint:

$$
x_R^T F x_L = 0
$$

where $F$ = **fundamental matrix** (encodes relative pose between two cameras).

If intrinsics $K_L, K_R$ are known:

$$
E = K_R^T F K_L
$$

is the **essential matrix**.

üëâ Epipolar lines reduce correspondence search to **1D along a line**.

---

### üèó Structure from Motion (SfM)

* Multiple images from **a moving camera**.
* Recover both:

  * 3D scene structure.
  * Camera trajectory.

Pipeline:

1. Feature matching across frames.
2. Estimate **relative pose** (epipolar geometry).
3. Triangulate 3D points.
4. Optimize all poses + points with **bundle adjustment**.

Mathematical model:

$$
x_{ij} \sim P_i X_j
$$

where $x_{ij}$ = image of 3D point $X_j$ in camera $i$.

---

### üåç Camera Networks and Panoramic Cameras

* **Camera arrays**: multiple fixed cameras ‚Üí reconstruct wide scenes.
* **Panoramic cameras**: wide-angle or stitched multi-camera systems.
* Geometry modeled with **nonlinear projection functions** (e.g., fisheye).

---

## ‚úÖ Wrap-Up of Part IV

* **Lens distortion**: radial & tangential effects, corrected via calibration.
* **Sensors**: pixel sampling, noise, quantization, rolling shutter artifacts.
* **Multi-camera geometry**: stereo (epipolar constraint), SfM, panoramic systems.

This bridges theory (ideal models) with **real-world considerations** ‚Äî preparing us for **Part V: Camera Calibration** (estimating intrinsics, extrinsics, and distortion).

---












Great ‚Äî let‚Äôs now dive into **Part V: Camera Calibration**, where theory meets practice.
This section is about how we **estimate** a real camera‚Äôs parameters (intrinsics, extrinsics, distortions) from images.

---

# üì∑ Part V: Camera Calibration

---

## 14. **Why Calibration Matters**

A camera calibration answers two questions:

1. **Intrinsics**: *How does the camera map 3D rays into 2D pixels?*

   * Focal lengths $(f_x, f_y)$
   * Principal point $(c_x, c_y)$
   * Skew, aspect ratio
   * Lens distortion parameters

2. **Extrinsics**: *Where is the camera in the world?*

   * Rotation matrix $R$
   * Translation vector $t$

---

### üîé Applications

* **3D Measurement** ‚Äî turn pixels into metric distances.
* **Pose Estimation** ‚Äî place AR/VR objects correctly in the real world.
* **3D Reconstruction** ‚Äî triangulate 3D points from multiple images.
* **Robotics & Navigation** ‚Äî localize a robot in its environment.

üëâ Without calibration, pixel coordinates have no physical meaning.

---

## 15. **Linear Camera Calibration**

The **Direct Linear Transform (DLT)** algorithm provides a first estimate of the projection matrix $P$.

---

### 15.1 Projection Equation

For a 3D point $X = (X,Y,Z,1)^T$:

$$
x \sim P X, \quad P \in \mathbb{R}^{3 \times 4}
$$

Let image point be $x = (u,v,1)^T$.
In inhomogeneous form:

$$
u = \frac{p_1^T X}{p_3^T X}, \quad v = \frac{p_2^T X}{p_3^T X}
$$

where $p_i^T$ are rows of $P$.

---

### 15.2 Linear Constraints

Each correspondence $(X, (u,v))$ gives two equations:

$$
u (p_3^T X) - (p_1^T X) = 0
$$

$$
v (p_3^T X) - (p_2^T X) = 0
$$

Stacking all equations:

$$
A \, \text{vec}(P) = 0
$$

where $A$ is a $2n \times 12$ matrix (for $n$ correspondences).

---

### 15.3 Solving with SVD

* Solve $\min ||A p||$ s.t. $\|p\|=1$.
* Solution = last singular vector of $A$.
* Reshape into $P$.

---

### 15.4 Requirements & Degenerate Cases

* Need at least **6 correspondences** (gives 12 equations).
* Points must not be **coplanar** (otherwise rank deficiency).

üëâ DLT gives $P$ up to a global scale.

---

## 16. **Nonlinear Optimization (Bundle Adjustment)**

Linear calibration is noisy. We refine by minimizing **reprojection error**.

---

### 16.1 Reprojection Error

Given parameters $(K, R, t, d)$ and 3D point $X$:

Predicted image point:

$$
\hat{x}(X) = \pi(K, R, t, d; X)
$$

Error for one correspondence:

$$
e = \| x - \hat{x}(X) \|^2
$$

Total error:

$$
E = \sum_{i,j} \| x_{ij} - \pi(P_i, X_j) \|^2
$$

where $i$ indexes cameras, $j$ points.

---

### 16.2 Optimization

* Nonlinear least squares (e.g., **Levenberg‚ÄìMarquardt**)
* Parameters optimized:

  * Intrinsics ($K$)
  * Extrinsics ($R,t$)
  * Distortion coefficients ($k_1, k_2, p_1, p_2,\dots$)

üëâ This is called **bundle adjustment** when optimizing over many cameras and points simultaneously.

---

## 17. **Practical Calibration Pipelines**

Now, how do we actually calibrate a real camera?

---

### 17.1 Checkerboard Calibration

1. Print a checkerboard with known square size.
2. Capture multiple images from different orientations.
3. Detect corner points in each image.
4. Build 2D‚Äì3D correspondences:

   * 3D = grid corners in world coordinates.
   * 2D = detected pixel locations.
5. Run DLT ‚Üí initial $P$.
6. Refine with nonlinear optimization.

---

### 17.2 OpenCV Implementation

OpenCV provides:

* `cv::findChessboardCorners()` ‚Üí detect corners.
* `cv::calibrateCamera()` ‚Üí estimates intrinsics, extrinsics, distortion.
* Returns:

  * $K$
  * Distortion coefficients ($k_1, k_2, p_1, p_2, \dots$)
  * Per-image extrinsics ($R_i, t_i$)

---

### 17.3 Evaluating Calibration Accuracy

1. **Reprojection Error**

   * Compute average pixel error between observed vs. predicted corners.
   * Good calibration: RMS error $\lt 0.5$ px (depends on resolution).

2. **Cross-validation**

   * Exclude some images ‚Üí test calibration on unseen ones.

3. **Stability**

   * Use many images (‚â•20).
   * Ensure board covers full field of view.

---

## ‚úÖ Wrap-Up of Part V

* **Calibration = estimating camera intrinsics, extrinsics, and distortion.**
* **DLT** provides a linear initialization.
* **Bundle adjustment** refines via nonlinear optimization.
* **Practical pipeline** uses checkerboards and OpenCV tools.
* Calibration quality = low reprojection error, stable across test images.

---




---

# üì∑ Part VI: Advanced Topics in Camera Models

---

## 18. **Special Camera Models**

Classical pinhole cameras assume a limited field of view (‚âà90¬∞). In practice, wide-angle, fish-eye, and mirror-based cameras expand this field or warp rays in special ways.

---

### 18.1 Fish-eye Cameras

* **Goal**: Very wide FOV (‚âà180¬∞ or more).
* Rays are mapped to the sensor using **nonlinear projection functions**.

#### Models of Projection

Let incoming ray have spherical coordinates:

$$
\theta = \text{angle from optical axis}, \quad \phi = \text{azimuth}
$$

Common fish-eye mappings (radial models):

1. **Equidistant projection**

   $$
   r = f \theta
   $$

2. **Equisolid angle projection**

   $$
   r = 2f \sin\left(\tfrac{\theta}{2}\right)
   $$

3. **Stereographic projection**

   $$
   r = 2f \tan\left(\tfrac{\theta}{2}\right)
   $$

4. **Orthographic projection**

   $$
   r = f \sin\theta
   $$

Here $r$ is the radial distance of the pixel from the principal point.

**Calibration** requires fitting the mapping function (OpenCV supports fisheye calibration).

---

### 18.2 Catadioptric Cameras

* Use **lenses + curved mirrors**.
* Provide a **single effective viewpoint** (SVP) if designed properly (paraboloidal mirrors).

#### Unified Sphere Model

* Each 3D ray is first mapped to a unit sphere around the camera.
* Projection:

  $$
  x = \frac{X}{Z+\xi \sqrt{X^2+Y^2+Z^2}}, \quad 
  y = \frac{Y}{Z+\xi \sqrt{X^2+Y^2+Z^2}}
  $$

where $\xi$ encodes mirror shape.

* Special cases: $\xi=0$ reduces to pinhole.

---

### 18.3 Omnidirectional Cameras

* Cover full $360^\circ$ panorama.
* Often modeled as mapping to a **sphere** or **cylinder**.
* Images are later **unwrapped** into panoramic coordinates.

Mathematically:

For spherical projection, a ray $(X,Y,Z)$ maps to longitude/latitude:

$$
u = \arctan2(Y,X), \quad v = \arccos\left(\tfrac{Z}{\sqrt{X^2+Y^2+Z^2}}\right)
$$

Then rescaled into an equirectangular image.

---

## 19. **Camera Model Extensions**

---

### 19.1 Projective Ambiguity & Self-Calibration

Recall projection matrix:

$$
x \sim P X, \quad P = K [R|t]
$$

If $H$ is any invertible $4 \times 4$ matrix:

$$
P' = P H, \quad X' = H^{-1} X
$$

Then:

$$
x \sim P X = P' X'
$$

üëâ This means reconstructions are only determined **up to a projective transformation** unless metric constraints are imposed.

**Self-calibration**: estimating intrinsics directly from multiple uncalibrated images, using constraints like:

$$
\omega = K^{-\top} K^{-1}
$$

where $\omega$ is the **Image of the Absolute Conic (IAC)**, invariant under projective transforms.

---

### 19.2 Multi-View Geometry Basics

Two cameras: $P, P'$.

For a 3D point $X$:

$$
x \sim P X, \quad x' \sim P' X
$$

Eliminating $X$ yields the **epipolar constraint**:

$$
x'^T F x = 0
$$

where $F$ = **fundamental matrix** ($3\times3$, rank 2).

If intrinsics are known:

$$
E = K'^T F K
$$

is the **essential matrix**.

$$
x'^T E x = 0
$$

Decomposing $E$ yields the **relative pose** ($R,t$) between two cameras.

---

### 19.3 Absolute vs. Relative Camera Pose

* **Relative pose**: Pose of camera 2 w\.r.t. camera 1 (retrieved from $E$).
* **Absolute pose**: Pose in a global world coordinate frame (requires extrinsics or known 3D‚Äì2D correspondences).

**Perspective-n-Point (PnP)** problem:
Given $n$ 3D‚Äì2D matches, solve for absolute pose $(R,t)$.

---

## 20. **Modern Applications of Camera Models**

---

### 20.1 Augmented Reality & Pose Estimation

* Goal: Align 3D graphics with real images.
* Requires **accurate extrinsics** (pose).
* Solved via:

  * Marker-based tracking (e.g., ARToolKit)
  * Natural feature-based SLAM

Equation:

$$
\hat{x} = K [R|t] X
$$

If $\hat{x}$ aligns with detected image features ‚Üí correct pose.

---

### 20.2 SLAM & Visual Odometry

**SLAM (Simultaneous Localization and Mapping)**

* Camera moves through environment.
* Extract features ‚Üí track across frames.
* Use **triangulation** + **bundle adjustment**.

VO/SLAM core equation:

$$
E = \sum_{i,j} \| x_{ij} - \pi(K, R_i, t_i, X_j)\|^2
$$

Minimized over poses $(R_i, t_i)$ and 3D points $X_j$.

---

### 20.3 Neural Rendering (NeRFs, Differentiable Cameras)

* **Neural Radiance Fields (NeRFs):** Represent scenes as a neural function $F_\theta(X, d) \to (c, \sigma)$.
* Rendering requires **camera projection models** to cast rays.

For each pixel $(u,v)$:

1. Convert to camera ray:

   $$
   \mathbf{r}(t) = C + t R K^{-1} (u,v,1)^T
   $$

   where $C$ = camera center.

2. Sample points along ray ‚Üí feed into MLP.

3. Volume render:

   $$
   C(u,v) = \int T(t) \, \sigma(t) \, c(t) \, dt
   $$

This is why differentiable camera models are crucial: gradients flow back through projection equations during training.

---

# ‚úÖ Wrap-Up of Part VI

* Special models (fisheye, catadioptric, omni) extend beyond pinhole.
* Multi-view geometry generalizes projection across multiple cameras.
* Projective ambiguity means reconstructions are relative ‚Äî calibration is needed for metric accuracy.
* Modern CV/AI applications (AR, SLAM, NeRFs) all fundamentally rely on **accurate camera modeling**.

