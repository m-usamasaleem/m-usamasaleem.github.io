
## 📌 Goal

Map a point

$$
X = (X, Y, Z, 1)^T  \quad \text{(in world coordinates)}
$$

onto

$$
x = (u, v, 1)^T  \quad \text{(pixel / image coordinates)}
$$

using a **perspective camera model** (central/pinhole projection).

---

## 🔁 Four Coordinate Systems

| Coordinate System  | Description                        |
| ------------------ | ---------------------------------- |
| **World**          | 3D coordinate frame of your scene  |
| **Camera**         | Origin at optical center of camera |
| **Image Plane**    | 2D plane inside camera (in meters) |
| **Sensor (Pixel)** | Discrete pixel grid (u, v)         |

---

## 🔧 Step 1 – Transform world point → camera coordinates

We need to describe where the **camera is located** and how it is **oriented** in the world.

$$
X_c = \underbrace{R}_{3\times3} \; X_w + \underbrace{t}_{3\times1}
$$

* $R$: rotation matrix (camera orientation)
* $t$: translation (camera center position in world coordinates)

👉 This conversion uses the **extrinsic parameters.**

In homogeneous form:

$$
X_c = [R \; | \; t] \cdot X_w
$$

---

## 🔭 Step 2 – Central (perspective) projection onto the image plane

Given camera coordinates $X_c = (x_c, y_c, z_c)$,

$$
x_{img} = \left( \frac{x_c}{z_c}, \; \frac{y_c}{z_c}, \; 1 \right)^T
$$

This is **where light rays hit the image plane**.
Note the division by $z_c$ ⇒ nonlinear (perspective).

---

## 📏 Step 3 – Convert image-plane coordinates → pixel coordinates

This depends on the internal geometry of the camera: focal length, pixel sizes, skew, optical center, etc. These are the **intrinsic parameters**, encoded into matrix $K$:

$$
K =
\begin{bmatrix}
f_x & s & c_x \\
0 & f_y & c_y \\
0 & 0 & 1
\end{bmatrix}
$$

* $f_x, f_y$: effective focal lengths in pixel units
* $(c_x, c_y)$: principal point (optical center in pixels)
* $s$: skew (usually 0)

Pixel coordinates:

$$
x = K \cdot x_{img}
$$

---

## ✅ Final Combined Mapping

All three steps can be merged into one matrix equation:

$$
x \sim P \cdot X_w  \quad \text{with} \quad P = K \,[R \;|\; t]
$$

* $P \in \mathbb{R}^{3\times4}$: **camera projection matrix**
* $X_w \in \mathbb{R}^{4}$: homogeneous world point
* $x \in \mathbb{R}^{3}$: homogeneous pixel coordinate
* “$\sim$” means equality up to scaling ⇒ you divide by the last coordinate to get (u,v)

---

## 🔄 Why can’t we invert this?

Because projection collapses 3D depth onto 2D — so information *is lost*. From a single image you know only that the 3D point lies somewhere on a **ray** starting from the camera center through the pixel. You cannot recover the depth without:

* Another camera (stereo/structure-from-motion)
* Or prior knowledge

---

## 🎯 Summary Table

| Transformation       | Matrix        | Parameters          |                   |
| -------------------- | ------------- | ------------------- | ----------------- |
| World → Camera       | ( \[R         | t] )                | Extrinsic (6 DOF) |
| Camera → Image plane | divide by $z$ | Perspective         |                   |
| Image plane → Pixels | $K$           | Intrinsic (4–5 DOF) |                   |
| **Full projection**  | $P$           | 11 DOF total        |                   |

---




## ✅ **Introduction: What is a Camera Model?**

A **camera model** is a *mathematical description* of how a 3D point in the world is projected onto a 2D image captured by a camera.

It serves two major purposes:

* To **simulate** how a camera sees the world.
* To **recover 3D information** from images.

Two types of parameters define a camera model:

| Parameter Type | Meaning                           |
| -------------- | --------------------------------- |
| **Extrinsic**  | Where the camera *is* and *looks* |
| **Intrinsic**  | How the camera *maps 3D → 2D*     |

---

## 📍 **Extrinsic Parameters** — *Where is the camera in the world?*

Extrinsics describe the **position and orientation** of the camera in a 3D world coordinate system.

* **3D Translation (t)** → Camera location in space (X, Y, Z).
* **3D Rotation (R)** → Camera orientation: what direction is it facing.

Together they form a 6-DoF (degrees of freedom) description:

$$
[R | t] \quad \text{(3×3 rotation + 3×1 translation)}
$$

👉 The **projection center** is the pinhole point where all rays intersect. Extrinsics tell us where this point lies in the world and which way the camera looks.

---

## 🔧 **Intrinsic Parameters** — *How does the camera map 3D → 2D pixels?*

Intrinsics specify the **internal geometry** of the camera and the sensor. Assuming the camera is sitting at the origin and looking straight ahead, the intrinsics define how a 3D point is projected onto the **image plane** and then converted into **pixel coordinates**.

Typical intrinsic parameters:

| Symbol     | Meaning                                             |
| ---------- | --------------------------------------------------- |
| $f_x, f_y$ | Focal lengths in x and y (usually in pixels)        |
| $c_x, c_y$ | Principal point on the image (optical center)       |
| $s$        | Skew angle between image axes (≈0 for most cameras) |

They are assembled in the **calibration matrix**:

$$
K =
\begin{bmatrix}
f_x & s & c_x \\
0   & f_y & c_y \\
0   & 0   & 1
\end{bmatrix}
$$

> Intrinsics = *4–5 parameters*, depending on whether skew is considered.

---

## 🔢 **Projection Equation: Combining Extrinsic and Intrinsic Parameters**

To map a world point $X = (X,Y,Z,1)^T$ to an image point $x = (u,v,1)^T$, we use:

$$
x \sim P \cdot X
$$

where

$$
P = K [R|t] \quad \text{is the } 3 \times 4 \text{ **projection matrix**}
$$

* **11 total degrees of freedom:**
  6 (extrinsic) + 5 (intrinsic)
* Multiplication produces a homogeneous pixel coordinate, which we divide by the last component to get $u,v$.

> ⚠️ This mapping is **not invertible** — 3D → 2D loses depth. A pixel corresponds to a *ray* in 3D space.

---

## 🎯 **Direct Linear Transform (DLT): Estimating Camera Matrix $P$**

The **DLT** algorithm is often used to compute the matrix $P$ *from correspondences*. Given ≥6 known 3D points and their matching 2D projections, we can solve for $P$ (up to scale) using linear algebra.

Once $P$ is recovered, we can **factor it** into:

* Intrinsics $K$
* Rotation $R$
* Translation $t$

---

## 🔍 **Lens Distortion**

Real cameras are not perfect pinhole systems — lenses introduce **non-linear distortions**, e.g.:

| Type       | Effect on image     |
| ---------- | ------------------- |
| Barrel     | Lines bulge outward |
| Pincushion | Lines pinch inward  |

These distortions are modeled by extra parameters (radial/tangential) and are typically removed during **camera calibration** so that the remaining model behaves like an ideal pinhole camera.

---

## 🛠️ **Calibration vs. Localization**

| Task             | What is estimated                             |
| ---------------- | --------------------------------------------- |
| **Calibration**  | Intrinsic parameters (K), optional distortion |
| **Localization** | Extrinsic parameters (R, t)                   |

Calibration usually uses **checkerboard patterns** or known grids, while localization uses world landmarks.

---

## 📌 **Summary Table**

| Component             | Matrix | Parameters                     |                              |
| --------------------- | ------ | ------------------------------ | ---------------------------- |
| Extrinsics            | \[R    |  t]                            | 3 rotations + 3 translations |
| Intrinsics            | K      | $f_x,f_y,c_x,c_y,s$            |                              |
| Full Projection       | P      | Combines both (11 DOF)         |                              |
| Distortion (optional) | D      | Radial / tangential parameters |                              |

---

## 🧭 Final Takeaway

A practical pinhole camera model is:

$$
x \sim K [R|t] X
$$

Once calibrated, this simple model forms the foundation for:

* Augmented reality
* 3D reconstruction
* Pose estimation
* Robotics & autonomous navigation

# 📷 **Comprehensive Tutorial on the Linear Camera Model & Camera Calibration**

---

## 🧩 **1. Overview: Forward Imaging Model (3D → 2D)**

The *forward imaging model* describes how a 3D point in the **world** projects onto a 2D **image**. It forms the mathematical foundation of **camera calibration**, where we estimate camera parameters.

We break this transformation into two parts:

| Stage                        | Purpose                            |
| ---------------------------- | ---------------------------------- |
| **World → Camera (3D → 3D)** | Camera **pose** (Extrinsics)       |
| **Camera → Image (3D → 2D)** | Camera **projection** (Intrinsics) |

---

## 🌍 **2. Camera Coordinate Frames**

We typically define two 3D coordinate systems:

* **World Frame (W):** fixed in the environment.
* **Camera Frame (C):** origin at the **projection center**, z-axis aligned with the **optical axis**.

A 3D point is:

* $P_W = (X_W, Y_W, Z_W)^T$ in world coordinates
* $P_C = (X_C, Y_C, Z_C)^T$ in camera coordinates

---

## 🔧 **3. Extrinsic Parameters: World → Camera**

Extrinsics describe *where the camera is* and *what direction it looks*. They define a rigid 3D transformation:

$$
P_C = R\,P_W + t
$$

* $R \in \mathbb{R}^{3×3}$: **rotation matrix**

  * Orthonormal (columns perpendicular, length 1)
  * $R^T R = I$, $R^{-1} = R^T$
* $t \in \mathbb{R}^{3×1}$: **translation vector**

> Together $[R|t]$ uses **6 degrees of freedom** (3 rotation + 3 translation).

**Homogeneous form:**

$$
\begin{bmatrix} P_C \\ 1 \end{bmatrix}
=
\underbrace{
\begin{bmatrix}
R & t \\
0 & 1
\end{bmatrix}}_{\text{Extrinsic matrix } E}
\begin{bmatrix}
P_W \\ 1
\end{bmatrix}
$$

---

## 🎯 **4. Perspective Projection: Camera → Image Plane (3D → 2D)**

Assume a pinhole model with focal length $f$:

$$
x_i = \frac{f X_C}{Z_C}, \quad y_i = \frac{f Y_C}{Z_C}
$$

These coordinates $(x_i, y_i)$ are in **meters or millimeters** on the *image plane* — not yet pixels!

---

## 🖼️ **5. Intrinsic Parameters: Image Plane → Pixel Coordinates**

To convert from physical image-plane coordinates to pixels on the image sensor:

* $m_x$, $m_y$: pixel density (pixels per mm)
* $(u_0, v_0)$: *principal point* (optical center in pixel coordinates)

$$
u = m_x \left( \frac{f X_C}{Z_C} \right) + u_0, \quad
v = m_y \left( \frac{f Y_C}{Z_C} \right) + v_0
$$

Let:

* $f_x = m_x \cdot f$
* $f_y = m_y \cdot f$

Then,

$$
u = \frac{f_x X_C}{Z_C} + u_0, \quad
v = \frac{f_y Y_C}{Z_C} + v_0
$$

These four quantities — $f_x, f_y, u_0, v_0$ — are the **intrinsic parameters**.

---

## 🚀 **6. Linear Projection in Homogeneous Coordinates**

Using homogeneous coordinates:

* 3D point: $P_C = (X_C, Y_C, Z_C, 1)^T$
* 2D pixel (homog.): $\tilde{x} = (u, v, 1)^T$

The intrinsic matrix is:

$$
K =
\begin{bmatrix}
f_x & 0   & u_0 \\
0   & f_y & v_0 \\
0   & 0   & 1
\end{bmatrix}
$$

Then:

$$
\tilde{x} \sim K \begin{bmatrix} I & 0 \end{bmatrix} P_C
\quad \Longrightarrow \quad
\tilde{x} \sim K [R|t] P_W
$$

Define the **projection matrix**:

$$
P = K [R|t] \quad (3 \times 4)
$$

Then:

$$
\tilde{x} \sim P \tilde{X}_W
$$

This is the *linear* camera model in homogeneous form.

---

## 🧠 **7. Camera Calibration**

**Goal:** Determine $P$, or equivalently $(K, R, t)$.

Two phases:

| Type      | Parameters        | Purpose           |
| --------- | ----------------- | ----------------- |
| Intrinsic | $f_x,f_y,u_0,v_0$ | Internal geometry |
| Extrinsic | $R,t$             | Camera pose       |

---

### 🔬 **7.1 Linear Calibration via DLT (Direct Linear Transform)**

Given at least **6 known 3D points** and their detected 2D pixel locations, we solve:

$$
\tilde{x}_i \sim P\,\tilde{X}_{W,i}
$$

* Each correspondence gives **2 linear equations**
* Stack ≥6 correspondences → solve for 12 unknowns of $P$ (up to scale)
* Solve using **SVD** → obtain $P$

---

### 🔧 **7.2 Decomposing $P$ to Intrinsics & Extrinsics**

Given $P$, factor into:

$$
P = K [R|t]
$$

* Use QR / RQ factorization
* Extract **upper-triangular $K$** (with positive diagonals)
* $R$ becomes orthonormal matrix
* $t$ extracted as offset

> This gives **complete camera calibration**, both intrinsics and extrinsics.

---

## 🎯 **8. Summary of Key Equations**

| Mapping           | Equation               |                  |
| ----------------- | ---------------------- | ---------------- |
| World → Camera    | $P_C = R P_W + t$      |                  |
| Camera → Pixel    | $\tilde{x} \sim K P_C$ |                  |
| Combined          | ( \tilde{x} \sim K\[R  | t]\tilde{X}\_W ) |
| Projection Matrix | ( P = K\[R             | t] )             |

---

## 🔍 **9. What About Lens Distortion?**

So far we modeled a perfect pinhole. Real lenses cause radial/tangential distortions ⇒ nonlinear parameters (e.g., $k_1, k_2, p_1, p_2$). These are estimated using nonlinear calibration after linear DLT initialization.

---

## 🧪 **10. Practical Calibration Pipeline (e.g., OpenCV)**

1. Print a checkerboard pattern
2. Capture multiple images of the board
3. Detect corner points in images
4. Use known 3D coordinates of board corners
5. Solve using DLT → initial $K, R, t$
6. Refine with nonlinear optimization (bundle adjustment) to also get distortion parameters

---

## ✅ **Conclusion**

The **linear camera model** provides a powerful way to relate 3D points to 2D pixels:

$$
\tilde{x} \sim P \tilde{X}_W
$$

Knowing the projection matrix $P$ means knowing *everything* about the camera. **Camera calibration** is the process of estimating this matrix, then decomposing it into the intrinsic and extrinsic parameters that define the camera’s geometry and pose.

