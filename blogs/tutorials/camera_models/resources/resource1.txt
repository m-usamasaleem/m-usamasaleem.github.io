
## ğŸ“Œ Goal

Map a point

$$
X = (X, Y, Z, 1)^T  \quad \text{(in world coordinates)}
$$

onto

$$
x = (u, v, 1)^T  \quad \text{(pixel / image coordinates)}
$$

using a **perspective camera model** (central/pinhole projection).

---

## ğŸ” Four Coordinate Systems

| Coordinate System  | Description                        |
| ------------------ | ---------------------------------- |
| **World**          | 3D coordinate frame of your scene  |
| **Camera**         | Origin at optical center of camera |
| **Image Plane**    | 2D plane inside camera (in meters) |
| **Sensor (Pixel)** | Discrete pixel grid (u, v)         |

---

## ğŸ”§ Step 1 â€“ Transform world point â†’ camera coordinates

We need to describe where the **camera is located** and how it is **oriented** in the world.

$$
X_c = \underbrace{R}_{3\times3} \; X_w + \underbrace{t}_{3\times1}
$$

* $R$: rotation matrix (camera orientation)
* $t$: translation (camera center position in world coordinates)

ğŸ‘‰ This conversion uses the **extrinsic parameters.**

In homogeneous form:

$$
X_c = [R \; | \; t] \cdot X_w
$$

---

## ğŸ”­ Step 2 â€“ Central (perspective) projection onto the image plane

Given camera coordinates $X_c = (x_c, y_c, z_c)$,

$$
x_{img} = \left( \frac{x_c}{z_c}, \; \frac{y_c}{z_c}, \; 1 \right)^T
$$

This is **where light rays hit the image plane**.
Note the division by $z_c$ â‡’ nonlinear (perspective).

---

## ğŸ“ Step 3 â€“ Convert image-plane coordinates â†’ pixel coordinates

This depends on the internal geometry of the camera: focal length, pixel sizes, skew, optical center, etc. These are the **intrinsic parameters**, encoded into matrix $K$:

$$
K =
\begin{bmatrix}
f_x & s & c_x \\
0 & f_y & c_y \\
0 & 0 & 1
\end{bmatrix}
$$

* $f_x, f_y$: effective focal lengths in pixel units
* $(c_x, c_y)$: principal point (optical center in pixels)
* $s$: skew (usually 0)

Pixel coordinates:

$$
x = K \cdot x_{img}
$$

---

## âœ… Final Combined Mapping

All three steps can be merged into one matrix equation:

$$
x \sim P \cdot X_w  \quad \text{with} \quad P = K \,[R \;|\; t]
$$

* $P \in \mathbb{R}^{3\times4}$: **camera projection matrix**
* $X_w \in \mathbb{R}^{4}$: homogeneous world point
* $x \in \mathbb{R}^{3}$: homogeneous pixel coordinate
* â€œ$\sim$â€ means equality up to scaling â‡’ you divide by the last coordinate to get (u,v)

---

## ğŸ”„ Why canâ€™t we invert this?

Because projection collapses 3D depth onto 2D â€” so information *is lost*. From a single image you know only that the 3D point lies somewhere on a **ray** starting from the camera center through the pixel. You cannot recover the depth without:

* Another camera (stereo/structure-from-motion)
* Or prior knowledge

---

## ğŸ¯ Summary Table

| Transformation       | Matrix        | Parameters          |                   |
| -------------------- | ------------- | ------------------- | ----------------- |
| World â†’ Camera       | ( \[R         | t] )                | Extrinsic (6 DOF) |
| Camera â†’ Image plane | divide by $z$ | Perspective         |                   |
| Image plane â†’ Pixels | $K$           | Intrinsic (4â€“5 DOF) |                   |
| **Full projection**  | $P$           | 11 DOF total        |                   |

---
