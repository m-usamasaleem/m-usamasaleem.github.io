<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Camera Models: Mathematical Foundations</title>
  <meta name="description" content="Master the mathematical foundations of camera models">
  
  <!-- MathJax for LaTeX rendering -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['\\(','\\)'], ['$', '$']],
        displayMath: [['$$','$$']],
        processEscapes: true,
        processEnvironments: true,
        packages: ['base', 'ams', 'noerrors', 'noundefined']
      },
      options: {
        ignoreHtmlClass: 'tex2jax_ignore',
        processHtmlClass: 'tex2jax_process'
      },
      startup: {
        pageReady: () => {
          return MathJax.startup.defaultPageReady().then(() => {
            console.log('MathJax is loaded and ready');
          });
        }
      }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
  <style>
    /* Reset and base styles */
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      line-height: 1.7;
      color: #222;
      background: #fff;
      max-width: 820px;
      margin: 0 auto;
      padding: 20px;
    }

    /* Header */
    header {
      text-align: center;
      margin-bottom: 40px;
      padding: 20px 0;
      border-bottom: 2px solid #A6372A;
    }

    h1 {
      color: #A6372A;
      font-size: 2.5rem;
      font-weight: 700;
      margin-bottom: 10px;
    }

    .subtitle {
      color: #666;
      font-size: 1.1rem;
      font-weight: 400;
    }

    /* Navigation */
    nav {
      margin: 16px 0 24px;
      padding: 0;
    }

    nav a {
      color: #A6372A;
      text-decoration: none;
      margin-right: 20px;
      font-weight: 500;
    }

    nav a:hover {
      text-decoration: underline;
    }

    /* Main content */
    main {
      background: transparent;
      padding: 0;
      border-radius: 0;
      box-shadow: none;
      margin-bottom: 40px;
    }

    /* Typography */
    h2 {
      color: #A6372A;
      font-size: 1.8rem;
      font-weight: 600;
      margin: 30px 0 15px 0;
      padding-bottom: 10px;
      border-bottom: 1px solid #eee;
    }

    h3 {
      color: #444;
      font-size: 1.4rem;
      font-weight: 600;
      margin: 25px 0 10px 0;
    }

    h4 {
      color: #555;
      font-size: 1.2rem;
      font-weight: 600;
      margin: 20px 0 10px 0;
    }

    p {
      margin-bottom: 14px;
      font-size: 1.02rem;
      line-height: 1.8;
    }

    ul, ol {
      margin: 15px 0;
      padding-left: 30px;
    }

    li {
      margin-bottom: 8px;
      line-height: 1.6;
    }

    strong {
      color: #A6372A;
      font-weight: 600;
    }

    em {
      font-style: italic;
      color: #666;
    }

    /* Dividers */
    hr {
      border: none;
      height: 1px;
      background: #eee;
      margin: 30px 0;
    }

    /* Code */
    code {
      background: #f5f5f5;
      padding: 2px 6px;
      border-radius: 4px;
      font-family: 'Courier New', monospace;
      color: #e83e8c;
      font-size: 0.9rem;
    }

    pre {
      background: #f6f8fa;
      padding: 16px;
      border-radius: 6px;
      overflow-x: auto;
      margin: 18px 0;
    }

    pre code {
      background: none;
      padding: 0;
      color: #333;
    }

    /* Math styling */
    .math-display {
      text-align: center;
      margin: 20px 0;
      padding: 20px;
      background: #f8f9fa;
      border-radius: 8px;
      border-left: 4px solid #A6372A;
    }

    /* Tables */
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 18px 0;
      background: white;
      border: 1px solid #eee;
    }

    th {
      background: #A6372A;
      color: white;
      padding: 10px 12px;
      text-align: left;
      font-weight: 600;
    }

    td {
      padding: 10px 12px;
      border-bottom: 1px solid #eee;
    }

    tr:nth-child(even) {
      background: #fafafa;
    }

    /* Loading state */
    .loading {
      text-align: center;
      padding: 40px;
      color: #666;
    }

    .spinner {
      border: 3px solid #f3f3f3;
      border-top: 3px solid #A6372A;
      border-radius: 50%;
      width: 30px;
      height: 30px;
      animation: spin 1s linear infinite;
      margin: 0 auto 20px;
    }

    @keyframes spin {
      0% { transform: rotate(0deg); }
      100% { transform: rotate(360deg); }
    }

    /* Footer */
    footer {
      text-align: center;
      padding: 20px;
      color: #666;
      border-top: 1px solid #eee;
    }

    footer a {
      color: #A6372A;
      text-decoration: none;
    }

    footer a:hover {
      text-decoration: underline;
    }

    /* Responsive design */
    @media (max-width: 768px) {
      body {
        padding: 15px;
      }

      main {
        padding: 25px;
      }

      h1 {
        font-size: 2rem;
      }

      h2 {
        font-size: 1.5rem;
      }

      h3 {
        font-size: 1.2rem;
      }

      nav {
        text-align: center;
      }

      nav a {
        display: block;
        margin: 5px 0;
      }
    }

    @media (max-width: 480px) {
      body {
        padding: 10px;
      }

      main {
        padding: 20px;
      }

      h1 {
        font-size: 1.8rem;
      }

      h2 {
        font-size: 1.3rem;
      }

      h3 {
        font-size: 1.1rem;
      }

      ul, ol {
        padding-left: 20px;
      }
    }

    /* Print styles */
    @media print {
      body {
        background: white;
        max-width: none;
        padding: 0;
      }

      main {
        box-shadow: none;
        padding: 0;
      }

      nav, footer {
        display: none;
      }
    }
  </style>
</head>

<body>
  <header>
    <h1>Camera Models: Mathematical Foundations</h1>
    <p class="subtitle">A comprehensive guide to understanding camera geometry and projection</p>
  </header>

  <nav>
    <a href="/blogs/">‚Üê Back to Tutorials</a>
    <a href="/">Portfolio</a>
  </nav>

  <main>
    <div id="tutorial-content">
      <div class="loading">
        <div class="spinner"></div>
        <p>Loading tutorial content...</p>
      </div>
    </div>
  </main>

  <footer>
    <p>&copy; 2024 Muhammad Usama Saleem. <a href="/blogs/">Back to Tutorials</a></p>
  </footer>

  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
  <script id="tutorial-md" type="text/plain">
# üìñ Table of Contents ‚Äî Understanding Camera Models in Detail

---

## Part I: Foundations of Camera Models

1. **Introduction to Camera Models**
   * Why do we need camera models?
   * Applications in computer vision, robotics, AR/VR
   * Historical note: pinhole to modern digital cameras

2. **Geometry of Image Formation**
   * Light, rays, and projection
   * The concept of the optical center
   * The image plane vs. the sensor plane

3. **Homogeneous Coordinates**
   * Why we use homogeneous coordinates
   * Representing points, lines, and transformations
   * Projective geometry basics

---

## Part II: Classical Camera Models

4. **The Pinhole Camera Model**
   * Basic setup and assumptions
   * Perspective projection equations
   * Image inversion and virtual image plane
   * Aperture size trade-offs

5. **Lens-Based Models**
   * The thin lens equation
   * Depth of field and focus
   * Lens distortions (barrel, pincushion, tangential)

6. **Alternative Simplified Models**
   * Weak perspective model
   * Orthographic projection
   * Scaled orthographic (paraperspective)
   * When approximations are useful

---

## Part III: Mathematical Formulation of Camera Projection

7. **Extrinsic Parameters (World ‚Üí Camera)**
   * Camera coordinate system
   * Rotation matrix $R$
   * Translation vector $t$
   * Rigid body transformation

8. **Intrinsic Parameters (Camera ‚Üí Pixels)**
   * Effective focal lengths $f_x, f_y$
   * Principal point $(c_x, c_y)$
   * Skew and pixel aspect ratio
   * The calibration matrix $K$

9. **The Full Camera Matrix**
   * Derivation: $P = K [R|t]$
   * Properties and degrees of freedom
   * Geometric interpretation

10. **Projection in Homogeneous Coordinates**
    * Matrix form of projection
    * Perspective divide
    * From 3D points to 2D pixels

---

## Part IV: Practical Considerations in Real Cameras

11. **Lens Distortion Models**
    * Radial distortion
    * Tangential distortion
    * Modeling and correction

12. **Sensor Characteristics**
    * Discretization into pixels
    * Noise and quantization
    * Rolling shutter effects

13. **Multi-Camera Systems**
    * Stereo geometry and epipolar constraint
    * Structure from motion (SfM)
    * Camera networks and panoramic cameras

---

## Part V: Camera Calibration

14. **Why Calibration Matters**
    * Estimating intrinsics and extrinsics
    * Applications in measurement and 3D vision

15. **Linear Camera Calibration**
    * Direct Linear Transform (DLT)
    * Requirements and degenerate cases

16. **Nonlinear Optimization (Bundle Adjustment)**
    * Reprojection error minimization
    * Distortion parameter estimation

17. **Practical Calibration Pipelines**
    * Checkerboard calibration
    * OpenCV implementation
    * Evaluating calibration accuracy

---

## Part VI: Advanced Topics

18. **Special Camera Models**
    * Fish-eye cameras
    * Catadioptric (mirror + lens) systems
    * Omnidirectional cameras

19. **Camera Model Extensions**
    * Projective ambiguity and self-calibration
    * Multi-view geometry basics
    * Absolute vs. relative camera pose

20. **Modern Applications of Camera Models**
    * Augmented reality & pose estimation
    * SLAM and visual odometry
    * Neural rendering (NeRFs, differentiable cameras)

---

‚ö° This TOC builds from **basic geometry ‚Üí classical models ‚Üí full camera matrix ‚Üí calibration ‚Üí advanced models and applications**.

# üì∑ Part I: Foundations of Camera Models

---

## 1. **Introduction to Camera Models**

### üîç Why Do We Need Camera Models?

In computer vision, the camera is the interface between the **3D world** and a **2D digital image**. A **camera model** is a mathematical abstraction that describes this transformation.

Mathematically, a 3D point

$$
X = (X, Y, Z, 1)^T
$$

is projected to a 2D pixel

$$
x = (u, v, 1)^T
$$

through a **camera projection function**:

$$
x \sim P X
$$

where $P \in \mathbb{R}^{3 \times 4}$ is the **camera matrix**.

Without a model, tasks like 3D reconstruction, augmented reality overlays, or robot navigation would be impossible ‚Äî because we couldn't reason how the 2D images relate back to the 3D world.

**The Fundamental Problem:**
The camera projection is a **many-to-one mapping** - infinitely many 3D points can project to the same 2D pixel. This is why we need sophisticated algorithms and multiple views to recover 3D information.

**Example:**
Consider a point at $(X, Y, Z)$ and another at $(2X, 2Y, 2Z)$. Both project to the same pixel $(u, v)$ under perspective projection, demonstrating the loss of depth information.

### ‚öôÔ∏è Applications in Vision and Robotics

**Computer Vision Applications:**
* **3D Reconstruction**: Building 3D models from multiple 2D images
* **Depth Estimation**: Computing distance to objects using stereo or monocular cues
* **Object Recognition**: Understanding object pose and orientation in 3D space
* **Image Stitching**: Creating panoramas by understanding camera geometry

**Robotics Applications:**
* **Visual SLAM**: Simultaneous Localization and Mapping using cameras
* **Robot Navigation**: Path planning and obstacle avoidance
* **Manipulation**: Precise positioning of robotic arms using visual feedback
* **Autonomous Vehicles**: Understanding the 3D environment from camera feeds

**AR/VR Applications:**
* **Pose Estimation**: Tracking camera position for overlaying graphics
* **Spatial Mapping**: Understanding the physical environment
* **Hand Tracking**: Precise hand pose estimation for interaction
* **Eye Tracking**: Understanding user gaze direction

**Industrial/Medical Applications:**
* **Photogrammetry**: Creating accurate 3D measurements from images
* **Medical Imaging**: Understanding camera geometry in surgical navigation
* **Quality Control**: Precise measurement and inspection
* **Reverse Engineering**: Reconstructing 3D models from photographs

### üï∞ Historical Note: From Pinhole to Digital Cameras

**Ancient Origins (5th century BCE):**
* **Mozi (China)**: First documented description of pinhole camera effects
* **Aristotle (Greece)**: Observed that light passing through small holes creates inverted images
* **Camera Obscura**: Used by artists and scientists for centuries

**Renaissance Perspective (15th century):**
* **Leon Battista Alberti**: Published "De Pictura" (1435), formalizing perspective geometry
* **Filippo Brunelleschi**: Demonstrated mathematical principles of perspective in art
* **Linear Perspective**: Established the foundation for modern camera models

**19th‚Äì20th Century Developments:**
* **Photographic Film**: Daguerreotype (1839) to modern film cameras
* **Glass Lenses**: Replaced pinholes for better light collection
* **Optical Theory**: Maxwell, Helmholtz, and others developed wave optics
* **Photogrammetry**: Mathematical methods for 3D reconstruction from photos

**Modern Digital Era:**
* **CCD/CMOS Sensors**: Replaced film with digital sensors (1970s onwards)
* **Digital Signal Processing**: Enabled real-time camera calibration and correction
* **Computer Vision**: Algorithms for automatic camera modeling and calibration
* **Mobile Cameras**: Ubiquitous cameras with sophisticated computational photography

**The Mathematical Legacy:**
Despite technological advances, the **pinhole camera model** remains the mathematical foundation. Modern cameras add lens distortions and sensor characteristics, but the core projection geometry is unchanged.

### üî¨ Mathematical Foundations

**Coordinate Systems:**
* **World Coordinates**: Global reference frame $(X_w, Y_w, Z_w)$
* **Camera Coordinates**: Local camera frame $(X_c, Y_c, Z_c)$
* **Image Coordinates**: 2D image plane $(x_i, y_i)$
* **Pixel Coordinates**: Discrete sensor array $(u, v)$

**The Projection Pipeline:**
1. **World ‚Üí Camera**: Rigid body transformation $[R|t]$
2. **Camera ‚Üí Image**: Perspective projection $/Z_c$
3. **Image ‚Üí Pixels**: Intrinsic transformation $K$

**Degrees of Freedom:**
* **Extrinsic**: 6 DOF (3 rotation + 3 translation)
* **Intrinsic**: 5 DOF (2 focal lengths + 2 principal point + 1 skew)
* **Total**: 11 DOF up to scale

**Scale Ambiguity:**
The camera matrix is defined up to scale - multiplying $P$ by any non-zero scalar gives the same projection. This is why we need calibration or multiple views to recover metric scale.

---

## 2. **Geometry of Image Formation**

### üåû Light, Rays, and Projection

* Each **3D point** emits or reflects light rays in all directions.
* The **camera aperture (pinhole)** restricts rays so that exactly **one ray per 3D point** reaches the image plane.
* This ensures a **unique mapping**: one world point ‚Üí one image point.

**Mathematical Setup:**
Let the **camera coordinate system** have origin at the optical center $O$, with the $Z$-axis pointing forward.
A world point in camera coordinates is

$$
X_c = (X_c, Y_c, Z_c)^T
$$

The ray from $O$ through $X_c$ intersects the **image plane** at distance $f$ (focal length) along the $Z$-axis.

By similar triangles (see diagram placeholder below):

$$
x_i = \frac{f X_c}{Z_c}, \quad y_i = \frac{f Y_c}{Z_c}
$$

This is the **perspective projection equation**.

### üéØ The Concept of the Optical Center

* The **optical center** (camera center) is the point where all rays converge.
* In the pinhole model, it's a single idealized point; in real cameras, it approximates the entrance pupil center.

### üñº The Image Plane vs. the Sensor Plane

* In pure geometry, the **image plane** is placed *behind* the pinhole at distance $f$ ‚Üí inverted image.
* Equivalently, a **virtual image plane** in front of the pinhole yields the same math up to a reflection.
* In real cameras, the **sensor plane** (CCD/CMOS) is a discrete pixel array; mapping to pixels is handled by the **intrinsic matrix** $K$.

### üî¨ Ray Tracing Fundamentals

**Ray Definition:**
A ray is a straight line extending from a point in a specific direction. In camera geometry, we consider rays from the optical center through scene points.

**Ray Equation:**
For a ray from optical center $O$ through point $X_c$:
$$
\mathbf{r}(t) = O + t \cdot \frac{X_c - O}{\|X_c - O\|}
$$

where $t$ is the parameter along the ray.

**Ray-Image Intersection:**
The ray intersects the image plane at $Z = f$:
$$
t = \frac{f}{Z_c}
$$

Substituting back gives the projection equations:
$$
x_i = \frac{f X_c}{Z_c}, \quad y_i = \frac{f Y_c}{Z_c}
$$

### üåà Optical Principles

**Light Propagation:**
* Light travels in straight lines in homogeneous media
* Reflection follows the law of reflection: angle of incidence = angle of reflection
* Refraction follows Snell's law: $n_1 \sin \theta_1 = n_2 \sin \theta_2$

**Aperture Effects:**
* **Small aperture**: Sharp image but dim (fewer rays)
* **Large aperture**: Bright image but blurred (multiple rays per point)
* **Optimal aperture**: Balance between brightness and sharpness

**Depth of Field:**
* Only objects at the focal distance are perfectly sharp
* Objects closer or farther blur into circles of confusion
* Depth of field increases with smaller aperture

### üìê Mathematical Derivation of Projection

**Step 1: Ray Direction**
Given a 3D point $X_c = (X_c, Y_c, Z_c)^T$ in camera coordinates, the ray direction is:
$$
\mathbf{d} = \frac{X_c}{\|X_c\|} = \frac{(X_c, Y_c, Z_c)^T}{\sqrt{X_c^2 + Y_c^2 + Z_c^2}}
$$

**Step 2: Ray Parameter**
The ray intersects the image plane at $Z = f$:
$$
\mathbf{r}(t) = (0, 0, 0)^T + t \cdot \mathbf{d} = (t \cdot d_x, t \cdot d_y, t \cdot d_z)^T
$$

Setting $Z = f$:
$$
t \cdot d_z = f \implies t = \frac{f}{d_z} = \frac{f \cdot \|X_c\|}{Z_c}
$$

**Step 3: Image Coordinates**
Substituting back:
$$
x_i = t \cdot d_x = \frac{f \cdot \|X_c\|}{Z_c} \cdot \frac{X_c}{\|X_c\|} = \frac{f X_c}{Z_c}
$$

$$
y_i = t \cdot d_y = \frac{f \cdot \|X_c\|}{Z_c} \cdot \frac{Y_c}{\|X_c\|} = \frac{f Y_c}{Z_c}
$$

### üîÑ Image Inversion and Virtual Image Plane

**Physical Image Plane (Behind Pinhole):**
* Image is inverted (upside down and left-right reversed)
* Mathematically: $x_i = -\frac{f X_c}{Z_c}, y_i = -\frac{f Y_c}{Z_c}$

**Virtual Image Plane (In Front):**
* Image has same orientation as scene
* Mathematically: $x_i = \frac{f X_c}{Z_c}, y_i = \frac{f Y_c}{Z_c}$

**Equivalence:**
Both models give equivalent results up to a coordinate transformation. The virtual image plane is often preferred for mathematical convenience.

### üéØ Optical Center in Real Cameras

**Pinhole Model:**
* Single point where all rays converge
* Mathematically idealized

**Real Lenses:**
* **Entrance Pupil**: The apparent aperture as seen from the object side
* **Exit Pupil**: The apparent aperture as seen from the image side
* **Nodal Points**: Points where light rays appear to cross without refraction

**Thin Lens Approximation:**
* Optical center at the center of the lens
* Valid when lens thickness is small compared to focal length

**Thick Lens Model:**
* Two principal planes
* Two nodal points
* More complex but more accurate for thick lenses

### üìä Sensor Characteristics

**Pixel Array:**
* Discrete grid of light-sensitive elements
* Each pixel has finite size and spacing
* Sampling converts continuous image to discrete pixels

**Pixel Response:**
* **Quantum Efficiency**: Fraction of photons converted to electrons
* **Fill Factor**: Fraction of pixel area that is light-sensitive
* **Dynamic Range**: Ratio of maximum to minimum detectable signal

**Color Sensing:**
* **Bayer Pattern**: Most common color filter array
* **RGB**: Red, Green, Blue filters
* **Demosaicing**: Interpolating full color from filtered data

### üîç Aperture Size Trade-offs

| Aperture Size | Brightness | Sharpness | Depth of Field |
|---------------|------------|-----------|----------------|
| **Small**     | Dim        | Sharp     | Large          |
| **Medium**    | Moderate   | Good      | Medium         |
| **Large**     | Bright     | Blurred   | Small          |

**Mathematical Relationship:**
* **F-number**: $f/\# = \frac{f}{D}$ where $D$ is aperture diameter
* **Exposure**: Proportional to $(f/\#)^{-2}$
* **Depth of Field**: Proportional to $f/\#$

**Optimal Aperture:**
* **Diffraction Limit**: Small apertures cause diffraction blur
* **Aberration Limit**: Large apertures cause lens aberrations
* **Sweet Spot**: Usually 2-3 stops down from maximum aperture

---

## 3. **Homogeneous Coordinates**

### ‚ö° Why Do We Use Homogeneous Coordinates?

Perspective projection involves **division by depth** $(Z_c)$:

$$
x_i = \frac{f X_c}{Z_c}, \quad y_i = \frac{f Y_c}{Z_c}
$$

This is nonlinear. But using **homogeneous coordinates**, we can write projection as a **linear matrix multiplication**.

### ‚úç Representing Points

* A 2D point $(u,v)$ is $(u,v,1)^T$.
* A 3D point $(X,Y,Z)$ is $(X,Y,Z,1)^T$.
* More generally: $ (x,y) \equiv (kx, ky, k) \ \forall k \neq 0 $ (defined up to scale).

### üìê Representing Lines and Transformations

* A 2D line $ax + by + c = 0$ is the vector $(a,b,c)^T$.
* Incidence: $l^T x = 0$ ‚áî point $x$ lies on line $l$.
* Geometric transforms (translation, rotation, projection) become matrices in homogeneous coordinates.

**Example: 2D translation by (t_x, t_y)**
```
T = [[1, 0, t_x],
     [0, 1, t_y],
     [0, 0,   1]]
x' = T x
```

### üåê Projective Geometry Basics

Homogeneous coordinates extend Euclidean geometry into **projective geometry**:

* **Points at infinity** and **vanishing points** are naturally represented.
* Camera projection matrix $P$ is a $3\times4$ projective transform, enabling the compact form $ \tilde{x} \sim P \tilde{X} $.

### üî¨ Mathematical Foundation

**Definition:**
Homogeneous coordinates represent points in projective space by adding an extra coordinate. A point $(x, y)$ in 2D becomes $(x, y, w)$ where $w \neq 0$.

**Equivalence Classes:**
Points $(x, y, w)$ and $(kx, ky, kw)$ represent the same point for any $k \neq 0$. This is written as $(x, y, w) \equiv (kx, ky, kw)$.

**Conversion:**
* **Euclidean to Homogeneous**: $(x, y) \rightarrow (x, y, 1)$
* **Homogeneous to Euclidean**: $(x, y, w) \rightarrow (x/w, y/w)$

### üìê 2D Homogeneous Coordinates

**Point Representation:**
* Euclidean point $(x, y)$ becomes $(x, y, 1)^T$
* Points at infinity have $w = 0$: $(x, y, 0)^T$

**Line Representation:**
* Line $ax + by + c = 0$ becomes vector $(a, b, c)^T$
* Line at infinity: $(0, 0, 1)^T$

**Point-Line Incidence:**
Point $x = (x, y, w)^T$ lies on line $l = (a, b, c)^T$ if and only if:
$$
l^T x = ax + by + cw = 0
$$

**Line Intersection:**
Two lines $l_1$ and $l_2$ intersect at point $x = l_1 \times l_2$

**Point Join:**
Two points $x_1$ and $x_2$ define line $l = x_1 \times x_2$

### üåç 3D Homogeneous Coordinates

**Point Representation:**
* Euclidean point $(X, Y, Z)$ becomes $(X, Y, Z, 1)^T$
* Points at infinity: $(X, Y, Z, 0)^T$

**Plane Representation:**
* Plane $aX + bY + cZ + d = 0$ becomes $(a, b, c, d)^T$

**Point-Plane Incidence:**
Point $X = (X, Y, Z, W)^T$ lies on plane $\pi = (a, b, c, d)^T$ if:
$$
\pi^T X = aX + bY + cZ + dW = 0
$$

### üîÑ Transformation Matrices

**2D Transformations:**

**Translation:**
$$
T = \begin{bmatrix} 
1 & 0 & t_x \\
0 & 1 & t_y \\
0 & 0 & 1
\end{bmatrix}
$$

**Rotation (by angle $\theta$):**
$$
R = \begin{bmatrix} 
\cos\theta & -\sin\theta & 0 \\
\sin\theta & \cos\theta & 0 \\
0 & 0 & 1
\end{bmatrix}
$$

**Scaling:**
$$
S = \begin{bmatrix} 
s_x & 0 & 0 \\
0 & s_y & 0 \\
0 & 0 & 1
\end{bmatrix}
$$

**Shear:**
$$
H = \begin{bmatrix} 
1 & h_x & 0 \\
h_y & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
$$

**3D Transformations:**

**Translation:**
$$
T = \begin{bmatrix} 
I_{3\times3} & t \\
0^T & 1
\end{bmatrix}
$$

**Rotation:**
$$
R = \begin{bmatrix} 
R_{3\times3} & 0 \\
0^T & 1
\end{bmatrix}
$$

**General Rigid Body:**
$$
\begin{bmatrix} 
R & t \\
0^T & 1
\end{bmatrix}
$$

### üéØ Projective Transformations

**2D Projective Transform (Homography):**
$$
H = \begin{bmatrix} 
h_{11} & h_{12} & h_{13} \\
h_{21} & h_{22} & h_{23} \\
h_{31} & h_{32} & h_{33}
\end{bmatrix}
$$

**Properties:**
* Preserves straight lines
* Maps points to points
* 8 degrees of freedom (up to scale)

**3D Projective Transform:**
$$
H = \begin{bmatrix} 
H_{3\times3} & t \\
v^T & s
\end{bmatrix}
$$

**Properties:**
* 15 degrees of freedom (up to scale)
* Preserves planes
* Maps lines to lines

### üåê Points at Infinity

**2D Points at Infinity:**
* Represented as $(x, y, 0)^T$
* Direction vector $(x, y)^T$
* All parallel lines intersect at the same point at infinity

**3D Points at Infinity:**
* Represented as $(X, Y, Z, 0)^T$
* Direction vector $(X, Y, Z)^T$
* All parallel lines in 3D intersect at the same point at infinity

**Vanishing Points:**
* Points where parallel lines appear to converge
* Represented as points at infinity in the image
* Used in perspective drawing and 3D reconstruction

### üîç Practical Examples

**Example 1: Translation**
Point $(2, 3)$ translated by $(1, 2)$:
$$
\begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 2 \\ 0 & 0 & 1 \end{bmatrix}
\begin{bmatrix} 2 \\ 3 \\ 1 \end{bmatrix} = 
\begin{bmatrix} 3 \\ 5 \\ 1 \end{bmatrix} \rightarrow (3, 5)
$$

**Example 2: Rotation**
Point $(1, 0)$ rotated by $90¬∞$:
$$
\begin{bmatrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}
\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix} = 
\begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix} \rightarrow (0, 1)
$$

**Example 3: Perspective Projection**
3D point $(X, Y, Z)$ projected to 2D:
$$
\begin{bmatrix} f & 0 & 0 \\ 0 & f & 0 \\ 0 & 0 & 1 \end{bmatrix}
\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} = 
\begin{bmatrix} fX \\ fY \\ Z \end{bmatrix} \rightarrow (fX/Z, fY/Z)
$$

### üé® Geometric Interpretation

**Projective Space:**
* Extends Euclidean space with points at infinity
* Provides unified framework for affine and projective transformations
* Enables linear representation of perspective projection

**Duality:**
* In 2D: points ‚Üî lines
* In 3D: points ‚Üî planes
* Duality principle: any theorem about points has a dual theorem about lines/planes

**Cross Ratio:**
* Projective invariant for four collinear points
* Preserved under projective transformations
* Used in camera calibration and 3D reconstruction

---

# üì∑ Part II: Classical Camera Models

---

## 4. **The Pinhole Camera Model**

### üìå Basic Setup and Assumptions

* Dark box, **tiny aperture** (pinhole), and an **image plane**.
* Aperture is *infinitesimal* (ideal), no lens refraction, purely perspective geometry.

### üìê Perspective Projection Equations

Let the image plane be at distance $f$.

$$
x_i = \frac{f X_c}{Z_c}, \quad y_i = \frac{f Y_c}{Z_c}
$$

### üîÑ Image Inversion and Virtual Image Plane

* Physical plane behind pinhole ‚Üí **inverted** image.
* Virtual plane in front ‚Üí same math without inversion.

### üîç Aperture Size Trade-offs

| Aperture Size | Brightness | Sharpness |
|---------------|------------|-----------|
| **Small**     | Dim        | Sharp (few rays) |
| **Large**     | Bright     | Blurred (many rays) |

This trade-off motivates **lenses**: more light while preserving focus.

### üî¨ Mathematical Derivation of Pinhole Projection

**Step 1: Ray Geometry**
Consider a 3D point $X_c = (X_c, Y_c, Z_c)^T$ in camera coordinates. The ray from the optical center $O = (0, 0, 0)^T$ through this point has direction:
$$
\mathbf{d} = \frac{X_c}{\|X_c\|} = \frac{(X_c, Y_c, Z_c)^T}{\sqrt{X_c^2 + Y_c^2 + Z_c^2}}
$$

**Step 2: Ray Parameterization**
The ray equation is:
$$
\mathbf{r}(t) = O + t \cdot \mathbf{d} = t \cdot \frac{X_c}{\|X_c\|}
$$

**Step 3: Image Plane Intersection**
The image plane is at $Z = f$. Setting the Z-component equal to $f$:
$$
t \cdot \frac{Z_c}{\|X_c\|} = f \implies t = \frac{f \cdot \|X_c\|}{Z_c}
$$

**Step 4: Projection Coordinates**
Substituting back into the ray equation:
$$
x_i = t \cdot \frac{X_c}{\|X_c\|} = \frac{f \cdot \|X_c\|}{Z_c} \cdot \frac{X_c}{\|X_c\|} = \frac{f X_c}{Z_c}
$$

$$
y_i = t \cdot \frac{Y_c}{\|X_c\|} = \frac{f \cdot \|X_c\|}{Z_c} \cdot \frac{Y_c}{\|X_c\|} = \frac{f Y_c}{Z_c}
$$

### üìä Properties of Pinhole Projection

**Linearity in Homogeneous Coordinates:**
In homogeneous coordinates, the projection becomes linear:
$$
\begin{bmatrix} x_i \\ y_i \\ 1 \end{bmatrix} \sim 
\begin{bmatrix} f & 0 & 0 & 0 \\ 0 & f & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix}
\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}
$$

**Depth Ambiguity:**
Points $(X, Y, Z)$ and $(kX, kY, kZ)$ project to the same image point for any $k > 0$:
$$
\frac{f \cdot kX}{kZ} = \frac{f X}{Z}, \quad \frac{f \cdot kY}{kZ} = \frac{f Y}{Z}
$$

**Back-Projection:**
Given an image point $(x_i, y_i)$, the corresponding 3D ray is:
$$
X_c(\lambda) = \lambda \cdot \begin{bmatrix} x_i/f \\ y_i/f \\ 1 \end{bmatrix}
$$

### üéØ Mathematical Analysis of Aperture Effects

**Geometric Blur:**
For a finite aperture of diameter $D$, the blur circle diameter $b$ at distance $Z$ is:
$$
b = \frac{D \cdot |Z - Z_f|}{Z_f}
$$

where $Z_f$ is the focal distance.

**Optimal Aperture Size:**
The optimal aperture balances geometric blur and diffraction:
$$
D_{opt} = \sqrt{\lambda f}
$$

where $\lambda$ is the wavelength of light.

**Depth of Field:**
The depth of field $\Delta Z$ is:
$$
\Delta Z = \frac{2 Z_f^2 \cdot c \cdot (f/\#)}{f^2 - c^2 \cdot (f/\#)^2}
$$

where $c$ is the circle of confusion diameter and $f/\#$ is the f-number.

### üîÑ Virtual vs. Physical Image Plane

**Physical Image Plane (Behind Pinhole):**
* Image coordinates: $x_i = -\frac{f X_c}{Z_c}, y_i = -\frac{f Y_c}{Z_c}$
* Image is inverted and left-right reversed
* Mathematically equivalent to virtual plane with coordinate transformation

**Virtual Image Plane (In Front):**
* Image coordinates: $x_i = \frac{f X_c}{Z_c}, y_i = \frac{f Y_c}{Z_c}$
* Image has same orientation as scene
* Preferred for mathematical convenience

**Equivalence Proof:**
The transformation between physical and virtual coordinates is:
$$
\begin{bmatrix} x_{virtual} \\ y_{virtual} \end{bmatrix} = 
\begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix}
\begin{bmatrix} x_{physical} \\ y_{physical} \end{bmatrix}
$$

### üìê Mathematical Limitations

**Singularities:**
* Points at $Z_c = 0$ (on the image plane) cannot be projected
* Points behind the camera ($Z_c < 0$) project to negative coordinates

**Approximations:**
* Assumes infinitesimal aperture (no diffraction)
* Ignores lens aberrations
* Assumes perfect geometric optics

**Error Analysis:**
For a real aperture of diameter $D$, the projection error is bounded by:
$$
|\Delta x_i| \leq \frac{D}{2} \cdot \frac{x_i}{Z_c}
$$

### üîç Advanced Mathematical Properties

**Projective Invariance:**
The pinhole projection preserves:
* Straight lines remain straight
* Incidence relationships
* Cross ratios

**Non-Invertibility:**
The projection is not invertible - multiple 3D points map to the same 2D point. The inverse mapping is a ray in 3D space.

**Homogeneous Form:**
The complete projection matrix in homogeneous coordinates is:
$$
P = \begin{bmatrix} f & 0 & 0 & 0 \\ 0 & f & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix}
$$

**Eigenvalues and Eigenvectors:**
* Eigenvalues: $[f, f, 1, 0]$
* The zero eigenvalue corresponds to the camera center
* The eigenvector $(0, 0, 0, 1)^T$ represents the camera center

### üìä Numerical Examples

**Example 1: Basic Projection**
Point $(10, 20, 50)$ with focal length $f = 25$:
$$
x_i = \frac{25 \cdot 10}{50} = 5, \quad y_i = \frac{25 \cdot 20}{50} = 10
$$

**Example 2: Depth Scaling**
Points $(10, 20, 50)$ and $(20, 40, 100)$:
* First point: $(5, 10)$
* Second point: $(5, 10)$ (same projection due to depth scaling)

**Example 3: Back-Projection**
Image point $(5, 10)$ with $f = 25$:
* Ray direction: $(5/25, 10/25, 1) = (0.2, 0.4, 1)$
* Any point on this ray projects to $(5, 10)$ 

---

## 5. **Lens-Based Models**

### üîé The Thin Lens Equation

Replacing the pinhole with a convex lens lets multiple rays converge to a point on the sensor.

$$
\frac{1}{f} = \frac{1}{z_o} + \frac{1}{z_i}
$$

* $f$: focal length, $z_o$: object distance, $z_i$: image distance.

### üéØ Depth of Field and Focus

* Only a specific distance is perfectly in focus; others blur to *circles of confusion*.
* DoF increases with smaller aperture; affected by focal length and sensor size.

### üìâ Lens Distortions

1. **Radial distortion** (barrel / pincushion):
   $$
   x_d = x (1 + k_1 r^2 + k_2 r^4 + \dots),\quad
   y_d = y (1 + k_1 r^2 + k_2 r^4 + \dots)
   $$

2. **Tangential distortion** (misalignment):
   $$
   x_d = x + [2 p_1 xy + p_2(r^2 + 2x^2)], \\
   y_d = y + [p_1(r^2 + 2y^2) + 2p_2 xy]
   $$

### üî¨ Mathematical Derivation of Thin Lens Equation

**Step 1: Ray Tracing Through Lens**
Consider a ray from object point $P_o$ at distance $z_o$ from the lens. The ray passes through the lens center and continues to image point $P_i$ at distance $z_i$.

**Step 2: Geometric Analysis**
Using similar triangles:
$$
\frac{h_o}{z_o} = \frac{h_i}{z_i}
$$

where $h_o$ is object height and $h_i$ is image height.

**Step 3: Lens Power**
The lens power $P$ is defined as:
$$
P = \frac{1}{f}
$$

**Step 4: Derivation**
For a thin lens, the lens equation follows from the geometric relationship:
$$
\frac{1}{z_o} + \frac{1}{z_i} = \frac{1}{f}
$$

**Alternative Derivation Using Snell's Law:**
At the lens surface, Snell's law gives:
$$
n_1 \sin \theta_1 = n_2 \sin \theta_2
$$

For small angles (paraxial approximation):
$$
n_1 \theta_1 = n_2 \theta_2
$$

This leads to the lens equation through geometric optics.

### üìä Magnification and Image Formation

**Lateral Magnification:**
$$
M = \frac{h_i}{h_o} = -\frac{z_i}{z_o}
$$

**Angular Magnification:**
$$
M_\alpha = \frac{\alpha_i}{\alpha_o} = \frac{z_o}{z_i}
$$

**Total Magnification:**
$$
M_{total} = M \cdot M_\alpha = -1
$$

**Image Formation Cases:**

1. **Real Object, Real Image** ($z_o > f$):
   * $z_i > 0$ (image on opposite side)
   * $M < 0$ (inverted image)

2. **Real Object, Virtual Image** ($z_o < f$):
   * $z_i < 0$ (image on same side)
   * $M > 0$ (upright image)

3. **Virtual Object** ($z_o < 0$):
   * Always forms real image
   * Used in compound lens systems

### üéØ Mathematical Analysis of Depth of Field

**Circle of Confusion:**
The diameter of the blur circle $c$ for a point at distance $Z$ from the focal plane is:
$$
c = \frac{D \cdot |Z - Z_f|}{Z_f}
$$

where $D$ is the aperture diameter and $Z_f$ is the focal distance.

**Depth of Field Calculation:**
The near and far limits of acceptable focus are:
$$
Z_{near} = \frac{Z_f \cdot (f^2 - c \cdot f \cdot N)}{f^2 + c \cdot f \cdot N}
$$

$$
Z_{far} = \frac{Z_f \cdot (f^2 + c \cdot f \cdot N)}{f^2 - c \cdot f \cdot N}
$$

where $N = f/D$ is the f-number.

**Total Depth of Field:**
$$
\Delta Z = Z_{far} - Z_{near} = \frac{2 \cdot Z_f^2 \cdot c \cdot N}{f^2 - c^2 \cdot N^2}
$$

**Hyperfocal Distance:**
The distance at which the far limit extends to infinity:
$$
H = \frac{f^2}{c \cdot N} + f
$$

### üìâ Mathematical Models of Lens Distortion

**Radial Distortion Model:**

**Barrel Distortion** ($k_1 < 0$):
$$
x_d = x \cdot (1 + k_1 r^2 + k_2 r^4 + k_3 r^6)
$$

$$
y_d = y \cdot (1 + k_1 r^2 + k_2 r^4 + k_3 r^6)
$$

where $r^2 = x^2 + y^2$.

**Pincushion Distortion** ($k_1 > 0$):
Same equations as barrel distortion but with positive coefficients.

**Mathematical Properties:**
* **Symmetry**: Distortion depends only on radial distance
* **Monotonicity**: Distortion increases with distance from center
* **Smoothness**: Continuous and differentiable

**Tangential Distortion Model:**

**Decentering Distortion:**
$$
x_d = x + [2p_1 xy + p_2(r^2 + 2x^2)]
$$

$$
y_d = y + [p_1(r^2 + 2y^2) + 2p_2 xy]
$$

**Mathematical Properties:**
* **Asymmetry**: Depends on both $x$ and $y$ coordinates
* **Quadratic**: Second-order polynomial in image coordinates
* **Decentering**: Models lens misalignment

### üîç Advanced Distortion Models

**Brown-Conrady Model:**
Combines radial and tangential distortion:
$$
x_d = x \cdot (1 + k_1 r^2 + k_2 r^4 + k_3 r^6) + [2p_1 xy + p_2(r^2 + 2x^2)]
$$

$$
y_d = y \cdot (1 + k_1 r^2 + k_2 r^4 + k_3 r^6) + [p_1(r^2 + 2y^2) + 2p_2 xy]
$$

**Fisheye Distortion:**
For wide-angle lenses:
$$
r_d = f \cdot \theta \cdot (1 + k_1 \theta^2 + k_2 \theta^4 + \dots)
$$

where $\theta = \arctan(r/f)$.

**Polynomial Distortion:**
General polynomial model:
$$
x_d = \sum_{i=0}^{n} \sum_{j=0}^{i} a_{ij} x^{i-j} y^j
$$

$$
y_d = \sum_{i=0}^{n} \sum_{j=0}^{i} b_{ij} x^{i-j} y^j
$$

### üìê Mathematical Calibration of Distortion

**Linear Calibration:**
For small distortions, linear approximation:
$$
\begin{bmatrix} x_d \\ y_d \end{bmatrix} = 
\begin{bmatrix} 1 + k_1 r^2 & 0 \\ 0 & 1 + k_1 r^2 \end{bmatrix}
\begin{bmatrix} x \\ y \end{bmatrix}
$$

**Nonlinear Calibration:**
Minimize reprojection error:
$$
\min_{k_1, k_2, p_1, p_2} \sum_i \|x_i - \pi(K, R, t, k_1, k_2, p_1, p_2; X_i)\|^2
$$

**Calibration Targets:**
* **Checkerboard**: Known 3D-2D correspondences
* **Circular targets**: Sub-pixel accuracy
* **Natural features**: Self-calibration

### üî¨ Numerical Examples

**Example 1: Thin Lens Calculation**
Object at $z_o = 100$mm, focal length $f = 50$mm:
$$
\frac{1}{100} + \frac{1}{z_i} = \frac{1}{50} \implies z_i = 100\text{mm}
$$

**Example 2: Magnification**
For the above case:
$$
M = -\frac{100}{100} = -1
$$

**Example 3: Radial Distortion**
Point $(100, 100)$ with $k_1 = 0.001$:
$$
r^2 = 100^2 + 100^2 = 20000
$$

$$
x_d = 100 \cdot (1 + 0.001 \cdot 20000) = 2100
$$

$$
y_d = 100 \cdot (1 + 0.001 \cdot 20000) = 2100
$$

**Example 4: Depth of Field**
$f = 50$mm, $N = 2.8$, $c = 0.03$mm, $Z_f = 1000$mm:
$$
\Delta Z = \frac{2 \cdot 1000^2 \cdot 0.03 \cdot 2.8}{50^2 - 0.03^2 \cdot 2.8^2} \approx 67\text{mm}
$$ 

---

# üìê Part III: Mathematical Formulation of Camera Projection

---

## 7. **Extrinsic Parameters (World ‚Üí Camera)**

### üìç Camera Coordinate System

* Origin at optical center, $Z$-axis forward, $X$ right, $Y$ down.

### üîÑ Rotation Matrix $R$

$$
X' = R X_W, \quad R \in SO(3), \ R^T R = I, \ \det(R)=+1
$$

### ‚ûï Translation Vector $t$

$$
X_C = R X_W + t
$$

### üß© Rigid Body Transformation

$$
\begin{bmatrix} X_C \\ 1 \end{bmatrix} =
\begin{bmatrix} R & t \\ 0^T & 1 \end{bmatrix}
\begin{bmatrix} X_W \\ 1 \end{bmatrix}
$$

### üî¨ Mathematical Properties of Rotation Matrices

**SO(3) Group Properties:**
* **Orthogonality**: $R^T R = RR^T = I$
* **Determinant**: $\det(R) = +1$
* **Inverse**: $R^{-1} = R^T$
* **Composition**: $R_1 R_2 \in SO(3)$

**Euler Angles Representation:**
Rotation around $Z$, then $Y$, then $X$:
$$
R = R_x(\alpha) R_y(\beta) R_z(\gamma)
$$

where:
$$
R_x(\alpha) = \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos\alpha & -\sin\alpha \\ 0 & \sin\alpha & \cos\alpha \end{bmatrix}
$$

$$
R_y(\beta) = \begin{bmatrix} \cos\beta & 0 & \sin\beta \\ 0 & 1 & 0 \\ -\sin\beta & 0 & \cos\beta \end{bmatrix}
$$

$$
R_z(\gamma) = \begin{bmatrix} \cos\gamma & -\sin\gamma & 0 \\ \sin\gamma & \cos\gamma & 0 \\ 0 & 0 & 1 \end{bmatrix}
$$

**Axis-Angle Representation:**
Rotation by angle $\theta$ around unit vector $\mathbf{u}$:
$$
R = I + \sin\theta \cdot [\mathbf{u}]_\times + (1-\cos\theta) \cdot [\mathbf{u}]_\times^2
$$

where $[\mathbf{u}]_\times$ is the skew-symmetric matrix:
$$
[\mathbf{u}]_\times = \begin{bmatrix} 0 & -u_z & u_y \\ u_z & 0 & -u_x \\ -u_y & u_x & 0 \end{bmatrix}
$$

**Quaternion Representation:**
Rotation represented by unit quaternion $q = (q_w, q_x, q_y, q_z)$:
$$
R = \begin{bmatrix} 
1-2q_y^2-2q_z^2 & 2(q_x q_y - q_w q_z) & 2(q_x q_z + q_w q_y) \\
2(q_x q_y + q_w q_z) & 1-2q_x^2-2q_z^2 & 2(q_y q_z - q_w q_x) \\
2(q_x q_z - q_w q_y) & 2(q_y q_z + q_w q_x) & 1-2q_x^2-2q_y^2
\end{bmatrix}
$$

### üìä Degrees of Freedom Analysis

**Rotation Matrix:**
* 9 elements but only 3 DOF due to constraints
* Constraints: $R^T R = I$ (6 equations)
* Free parameters: 3 (Euler angles, axis-angle, or quaternion)

**Translation Vector:**
* 3 DOF (x, y, z components)

**Total Extrinsic Parameters:**
* 6 DOF (3 rotation + 3 translation)

### üîç Numerical Examples

**Example 1: Rotation Matrix**
Rotation of $90¬∞$ around Z-axis:
$$
R = \begin{bmatrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}
$$

**Example 2: Translation**
Translation by $(1, 2, 3)$:
$$
t = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
$$

**Example 3: Combined Transformation**
Point $(10, 20, 30)$ in world coordinates:
$$
X_C = R X_W + t = \begin{bmatrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 10 \\ 20 \\ 30 \end{bmatrix} + \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} = \begin{bmatrix} -19 \\ 12 \\ 33 \end{bmatrix}
$$

---

## 8. **Intrinsic Parameters (Camera ‚Üí Pixels)**

### üéØ Effective Focal Lengths $f_x, f_y$

$$
f_x = m_x f,\quad f_y = m_y f
$$

### üìç Principal Point $(c_x, c_y)$

Intersection of optical axis with sensor; near image center but not exact.

### üîÄ Skew and Pixel Aspect Ratio

Skew $s = f_x \cot\theta$ if axes not perfectly orthogonal.

### üèó Calibration Matrix $K$

$$
K =
\begin{bmatrix}
f_x & s & c_x \\
0 & f_y & c_y \\
0 & 0 & 1
\end{bmatrix}
$$

### üî¨ Mathematical Derivation of Intrinsic Matrix

**Step 1: Physical Parameters**
* **Focal length**: $f$ (distance from optical center to image plane)
* **Pixel size**: $m_x, m_y$ (physical size of pixels in mm)
* **Principal point**: $(c_x, c_y)$ (intersection of optical axis with sensor)
* **Skew**: $s$ (non-orthogonality of pixel axes)

**Step 2: Coordinate Transformations**
From image coordinates $(x_i, y_i)$ to pixel coordinates $(u, v)$:

$$
u = m_x \cdot x_i + c_x = \frac{f_x \cdot X_c}{Z_c} + c_x
$$

$$
v = m_y \cdot y_i + c_y = \frac{f_y \cdot Y_c}{Z_c} + c_y
$$

**Step 3: Matrix Form**
In homogeneous coordinates:
$$
\begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = 
\begin{bmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix}
\begin{bmatrix} x_i \\ y_i \\ 1 \end{bmatrix}
$$

**Step 4: Including Skew**
For non-orthogonal pixel axes:
$$
\begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = 
\begin{bmatrix} f_x & s & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix}
\begin{bmatrix} x_i \\ y_i \\ 1 \end{bmatrix}
$$

### üìä Properties of Calibration Matrix

**Invertibility:**
$$
K^{-1} = \begin{bmatrix} 
\frac{1}{f_x} & -\frac{s}{f_x f_y} & \frac{s c_y - f_y c_x}{f_x f_y} \\
0 & \frac{1}{f_y} & -\frac{c_y}{f_y} \\
0 & 0 & 1
\end{bmatrix}
$$

**Determinant:**
$$
\det(K) = f_x f_y
$$

**Image of Absolute Conic:**
$$
\omega = K^{-T} K^{-1} = \begin{bmatrix} 
\frac{1}{f_x^2} & -\frac{s}{f_x^2 f_y} & \frac{s c_y - f_y c_x}{f_x^2 f_y} \\
-\frac{s}{f_x^2 f_y} & \frac{s^2 + f_y^2}{f_x^2 f_y^2} & \frac{s(c_x f_y - s c_y) - f_y^2 c_y}{f_x^2 f_y^2} \\
\frac{s c_y - f_y c_x}{f_x^2 f_y} & \frac{s(c_x f_y - s c_y) - f_y^2 c_y}{f_x^2 f_y^2} & \frac{(s c_y - f_y c_x)^2 + f_y^2 c_y^2}{f_x^2 f_y^2} + 1
\end{bmatrix}
$$

### üéØ Principal Point Estimation

**Geometric Method:**
* Find vanishing points of orthogonal lines
* Principal point is the intersection of lines connecting opposite vanishing points

**Algebraic Method:**
* Use the Image of Absolute Conic (IAC)
* Principal point is the pole of the line at infinity

**Statistical Method:**
* Minimize reprojection error over multiple images
* Principal point often near image center but not exactly

### üîç Skew Parameter Analysis

**Physical Meaning:**
* Skew models non-orthogonal pixel axes
* Common in CCD sensors with rectangular pixels
* Usually very small in modern cameras

**Mathematical Properties:**
* $s = f_x \cot\theta$ where $\theta$ is the angle between pixel axes
* For orthogonal axes: $s = 0$
* Skew affects the shape of the IAC

**Calibration:**
* Estimate from checkerboard calibration
* Often fixed to zero for simplicity
* Important for high-precision applications

### üìê Numerical Examples

**Example 1: Basic Intrinsic Matrix**
$f_x = 1000$, $f_y = 1000$, $c_x = 320$, $c_y = 240$:
$$
K = \begin{bmatrix} 1000 & 0 & 320 \\ 0 & 1000 & 240 \\ 0 & 0 & 1 \end{bmatrix}
$$

**Example 2: With Skew**
$f_x = 1000$, $f_y = 1000$, $s = 0.1$, $c_x = 320$, $c_y = 240$:
$$
K = \begin{bmatrix} 1000 & 0.1 & 320 \\ 0 & 1000 & 240 \\ 0 & 0 & 1 \end{bmatrix}
$$

**Example 3: Projection**
3D point $(10, 20, 50)$ with above intrinsic matrix:
$$
\begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = 
\begin{bmatrix} 1000 & 0 & 320 \\ 0 & 1000 & 240 \\ 0 & 0 & 1 \end{bmatrix}
\begin{bmatrix} 10/50 \\ 20/50 \\ 1 \end{bmatrix} = 
\begin{bmatrix} 520 \\ 640 \\ 1 \end{bmatrix}
$$

---

## 9. **The Full Camera Matrix**

### üßÆ Derivation: $P = K [R|t]$

Start with homogeneous world point $\tilde{X}_W = (X,Y,Z,1)^T$:

1. $\ X_C = [R|t]\tilde{X}_W$
2. $\ x_{img} = (X_C/Z_C, \ Y_C/Z_C, \ 1)^T$
3. $\ x = K x_{img}$

$$
x \sim K [R|t] \tilde{X}_W \ \Rightarrow \ P = K [R|t]
$$

### üìä Properties and Degrees of Freedom

* $K$: 5 DOF, $[R|t]$: 6 DOF ‚Üí $P$ has 11 DOF up to scale.

### üåê Geometric Interpretation

Each pixel corresponds to a ray $X_W(\lambda)=C+\lambda d$; projection collapses depth, so it's non-invertible.

### üî¨ Complete Mathematical Derivation

**Step 1: World to Camera Transformation**
$$
X_C = R X_W + t
$$

In homogeneous coordinates:
$$
\begin{bmatrix} X_C \\ 1 \end{bmatrix} = 
\begin{bmatrix} R & t \\ 0^T & 1 \end{bmatrix}
\begin{bmatrix} X_W \\ 1 \end{bmatrix}
$$

**Step 2: Perspective Projection**
$$
x_{img} = \frac{X_C}{Z_C}, \quad y_{img} = \frac{Y_C}{Z_C}
$$

In homogeneous coordinates:
$$
\begin{bmatrix} x_{img} \\ y_{img} \\ 1 \end{bmatrix} \sim 
\begin{bmatrix} X_C \\ Y_C \\ Z_C \end{bmatrix}
$$

**Step 3: Intrinsic Transformation**
$$
\begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = K 
\begin{bmatrix} x_{img} \\ y_{img} \\ 1 \end{bmatrix}
$$

**Step 4: Combined Transformation**
$$
\begin{bmatrix} u \\ v \\ 1 \end{bmatrix} \sim K 
\begin{bmatrix} X_C \\ Y_C \\ Z_C \end{bmatrix} = K [R|t] 
\begin{bmatrix} X_W \\ Y_W \\ Z_W \\ 1 \end{bmatrix}
$$

Therefore:
$$
P = K [R|t]
$$

### üìê Mathematical Properties of Camera Matrix

**Rank:**
* $\text{rank}(P) = 3$ (full rank)
* The null space of $P$ is the camera center

**Camera Center:**
The camera center $C$ satisfies $P C = 0$:
$$
C = -\begin{bmatrix} R^T t \\ 1 \end{bmatrix}
$$

**Projection Rays:**
Each pixel $(u, v)$ corresponds to a ray in 3D space:
$$
X(\lambda) = C + \lambda \cdot R^T K^{-1} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}
$$

**Epipolar Geometry:**
For two cameras with matrices $P_1$ and $P_2$:
$$
F = [P_2 C_1]_\times P_2 P_1^+
$$

where $P_1^+$ is the pseudo-inverse of $P_1$.

### üîç Decomposition of Camera Matrix

**QR Decomposition:**
$$
P = K [R|t] = \begin{bmatrix} Q & q \end{bmatrix} \begin{bmatrix} R & t \\ 0^T & 1 \end{bmatrix}
$$

where $Q$ is orthogonal and $R$ is upper triangular.

**SVD Decomposition:**
$$
P = U \Sigma V^T
$$

where $U \in \mathbb{R}^{3 \times 3}$ is orthogonal, $\Sigma \in \mathbb{R}^{3 \times 4}$ is diagonal, and $V \in \mathbb{R}^{4 \times 4}$ is orthogonal.

**Extraction of Parameters:**
From $P = K [R|t]$:
* $K$: Upper triangular part of $P$ after normalization
* $R$: Orthogonal part of $[R|t]$
* $t$: Translation part of $[R|t]$

### üìä Numerical Examples

**Example 1: Complete Camera Matrix**
Given:
$$
K = \begin{bmatrix} 1000 & 0 & 320 \\ 0 & 1000 & 240 \\ 0 & 0 & 1 \end{bmatrix}
$$

$$
R = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}, \quad t = \begin{bmatrix} 0 \\ 0 \\ -5 \end{bmatrix}
$$

Then:
$$
P = K [R|t] = \begin{bmatrix} 1000 & 0 & 320 & 0 \\ 0 & 1000 & 240 & 0 \\ 0 & 0 & 1 & -5 \end{bmatrix}
$$

**Example 2: Projection**
World point $(10, 20, 30)$:
$$
\begin{bmatrix} u \\ v \\ 1 \end{bmatrix} \sim 
\begin{bmatrix} 1000 & 0 & 320 & 0 \\ 0 & 1000 & 240 & 0 \\ 0 & 0 & 1 & -5 \end{bmatrix}
\begin{bmatrix} 10 \\ 20 \\ 30 \\ 1 \end{bmatrix} = 
\begin{bmatrix} 10320 \\ 20240 \\ 25 \end{bmatrix} \sim 
\begin{bmatrix} 412.8 \\ 809.6 \\ 1 \end{bmatrix}
$$

**Example 3: Camera Center**
For the above camera matrix:
$$
C = -\begin{bmatrix} R^T t \\ 1 \end{bmatrix} = -\begin{bmatrix} 0 \\ 0 \\ -5 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 5 \\ 1 \end{bmatrix}
$$

So the camera is at $(0, 0, 5)$ in world coordinates. 

---

## 10. **Projection in Homogeneous Coordinates**

### üßæ Matrix Form of Projection

$$
\tilde{x} =
\begin{bmatrix} u \\ v \\ w \end{bmatrix}
\sim
P
\begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}
$$

### ‚úÇÔ∏è Perspective Divide

$$
(u', v') = \left(\frac{u}{w}, \frac{v}{w}\right)
$$

### üì∏ From 3D Points to 2D Pixels

1. World ‚Üí camera via $[R|t]$
2. Perspective divide $/Z_c$
3. Intrinsics $K$ ‚Üí pixel $(u,v)$

### üî¨ Mathematical Analysis of Perspective Divide

**Why Perspective Divide is Necessary:**
The homogeneous projection gives us:
$$
\begin{bmatrix} u \\ v \\ w \end{bmatrix} = P \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}
$$

But we need Euclidean coordinates $(u', v')$ for the image. The perspective divide converts homogeneous to Euclidean:
$$
u' = \frac{u}{w}, \quad v' = \frac{v}{w}
$$

**Mathematical Properties:**
* **Scale Invariance**: $(u, v, w) \equiv (ku, kv, kw)$ for any $k \neq 0$
* **Singularity**: When $w = 0$, the point is at infinity
* **Linearity**: The projection matrix $P$ is linear in homogeneous coordinates

**Complete Projection Pipeline:**
$$
\begin{bmatrix} X_W \\ Y_W \\ Z_W \\ 1 \end{bmatrix} 
\xrightarrow{[R|t]} 
\begin{bmatrix} X_C \\ Y_C \\ Z_C \\ 1 \end{bmatrix}
\xrightarrow{\text{perspective}} 
\begin{bmatrix} X_C/Z_C \\ Y_C/Z_C \\ 1 \end{bmatrix}
\xrightarrow{K} 
\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}
$$

---

# üì∑ Part IV: Practical Considerations in Real Cameras

---

## 11. **Lens Distortion Models**

Ideal pinhole maps straight 3D lines to straight 2D lines; real lenses bend rays ‚Üí **distortion**.

### üîµ Radial Distortion

$$
r^2 = x^2 + y^2
$$

$$
x_d = x (1 + k_1 r^2 + k_2 r^4 + k_3 r^6), \quad
y_d = y (1 + k_1 r^2 + k_2 r^4 + k_3 r^6)
$$

### üî∂ Tangential Distortion

$$
x_d = x + [2p_1 xy + p_2(r^2 + 2x^2)], \\
y_d = y + [p_1(r^2 + 2y^2) + 2p_2 xy]
$$

### üõ† Modeling and Correction

Estimate $K$ and distortion parameters together (e.g., OpenCV `calibrateCamera`); correction uses inverse mapping (often iterative).

### üî¨ Mathematical Analysis of Distortion Models

**Radial Distortion Function:**
The radial distortion function $D(r)$ is:
$$
D(r) = 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 + \dots
$$

**Mathematical Properties:**
* **Symmetry**: $D(r) = D(-r)$ (radial symmetry)
* **Monotonicity**: Usually $D(r) > 1$ for barrel distortion, $D(r) < 1$ for pincushion
* **Smoothness**: Continuous and differentiable

**Inverse Distortion:**
For correction, we need the inverse function. For small distortions:
$$
D^{-1}(r) \approx 1 - k_1 r^2 - k_2 r^4 - k_3 r^6
$$

**Combined Distortion Model:**
The complete distortion model is:
$$
\begin{bmatrix} x_d \\ y_d \end{bmatrix} = 
\begin{bmatrix} x \\ y \end{bmatrix} \cdot D(r) + 
\begin{bmatrix} 2p_1 xy + p_2(r^2 + 2x^2) \\ p_1(r^2 + 2y^2) + 2p_2 xy \end{bmatrix}
$$

---

## 12. **Sensor Characteristics**

### üü¶ Discretization into Pixels

$$
u = \lfloor m_x x_i + c_x \rfloor,\quad v = \lfloor m_y y_i + c_y \rfloor
$$

Sampling causes aliasing on high-frequency patterns.

### üîä Noise and Quantization

* Photon shot, thermal, and readout noise.
* $b$-bit sensor ‚Üí intensities in $[0, 2^b-1]$.

### üé• Rolling Shutter Effects

Rows exposed sequentially; fast motion bends geometry.

$$
t_i = t_0 + i \Delta t \ \Rightarrow \ x(t), y(t) \sim P(t) X_W
$$

### üî¨ Mathematical Models of Sensor Noise

**Photon Shot Noise:**
Follows Poisson distribution:
$$
P(n) = \frac{\lambda^n e^{-\lambda}}{n!}
$$

where $\lambda$ is the expected number of photons.

**Readout Noise:**
Gaussian noise with standard deviation $\sigma_{read}$:
$$
I_{measured} = I_{true} + \mathcal{N}(0, \sigma_{read}^2)
$$

**Quantization Noise:**
For $b$-bit quantization:
$$
\sigma_{quant} = \frac{1}{\sqrt{12}} \cdot \frac{1}{2^b}
$$

**Total Noise Model:**
$$
\sigma_{total}^2 = \sigma_{shot}^2 + \sigma_{read}^2 + \sigma_{quant}^2
$$

---

## 13. **Multi-Camera Systems**

### üîÄ Stereo Geometry and Epipolar Constraint

$$
x_R^T F x_L = 0
$$

If intrinsics known, essential matrix $E = K_R^T F K_L$.

### üèó Structure from Motion (SfM)

$$
x_{ij} \sim P_i X_j
$$

Estimate relative poses, triangulate, then bundle adjust.

### üåç Camera Networks and Panoramic Cameras

Arrays and panoramic systems require nonlinear projection models (e.g., fisheye).

### üî¨ Mathematical Foundation of Stereo Geometry

**Epipolar Constraint:**
For corresponding points $x_L$ and $x_R$ in left and right images:
$$
x_R^T F x_L = 0
$$

where $F$ is the fundamental matrix.

**Fundamental Matrix Properties:**
* **Rank**: $\text{rank}(F) = 2$
* **Singularity**: $F$ is singular (det$(F) = 0$)
* **Epipolar Lines**: $l_R = F x_L$ and $l_L = F^T x_R$

**Essential Matrix:**
When camera intrinsics are known:
$$
E = K_R^T F K_L
$$

**Essential Matrix Decomposition:**
$$
E = U \text{diag}(1, 1, 0) V^T
$$

The relative pose $(R, t)$ can be extracted from $E$:
$$
R = U W V^T \text{ or } U W^T V^T
$$

$$
t = U \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}
$$

where $W = \begin{bmatrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}$.

**Triangulation:**
Given corresponding points and camera matrices, 3D point $X$ satisfies:
$$
x_L \sim P_L X, \quad x_R \sim P_R X
$$

This gives the linear system:
$$
\begin{bmatrix} x_L \times P_L \\ x_R \times P_R \end{bmatrix} X = 0
$$

---

# üì∑ Part V: Camera Calibration

---

## 14. **Why Calibration Matters**

**Intrinsics**
* Focal lengths $(f_x, f_y)$
* Principal point $(c_x, c_y)$
* Skew, aspect ratio
* Distortion parameters

**Extrinsics**
* Rotation $R$
* Translation $t$

**Applications**
* 3D measurement, pose estimation, reconstruction, robotics & navigation.

### üî¨ Mathematical Motivation for Calibration

**Metric Reconstruction:**
Without calibration, we can only reconstruct up to a projective transformation. With calibration, we can achieve metric reconstruction.

**Pose Estimation:**
For pose estimation, we need to solve:
$$
\min_{R,t} \sum_i \|x_i - \pi(K, R, t; X_i)\|^2
$$

**3D Measurement:**
Accurate 3D measurements require precise camera parameters:
$$
X = \frac{Z}{f} \cdot (u - c_x, v - c_y)
$$

---

## 15. **Linear Camera Calibration**

### 15.1 Projection Equation

$$
x \sim P X,\ \ P \in \mathbb{R}^{3 \times 4}
$$

In inhomogeneous form:
$ u = \frac{p_1^T X}{p_3^T X}, \ v = \frac{p_2^T X}{p_3^T X} $

### 15.2 Linear Constraints

$$
u (p_3^T X) - (p_1^T X) = 0,\quad
v (p_3^T X) - (p_2^T X) = 0
$$

Stack into $A\,\text{vec}(P)=0$ with $A\in \mathbb{R}^{2n\times12}$.

### 15.3 Solving with SVD

Solve $\min \|A p\|$ s.t. $\|p\|=1$; take last singular vector, reshape to $P$.

### 15.4 Requirements & Degenerate Cases

* ‚â• 6 correspondences (12 equations).
* Points must not be coplanar (rank deficiency otherwise).

### üî¨ Mathematical Derivation of DLT

**Step 1: Linearization**
From the projection equation:
$$
\begin{bmatrix} u \\ v \\ 1 \end{bmatrix} \sim 
\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\
p_{21} & p_{22} & p_{23} & p_{24} \\
p_{31} & p_{32} & p_{33} & p_{34} \end{bmatrix}
\begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}
$$

**Step 2: Cross Product**
Taking the cross product with the left side:
$$
\begin{bmatrix} u \\ v \\ 1 \end{bmatrix} \times P \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix} = 0
$$

This gives two independent equations:
$$
u \cdot (p_3^T X) - (p_1^T X) = 0
$$

$$
v \cdot (p_3^T X) - (p_2^T X) = 0
$$

**Step 3: Linear System**
For $n$ correspondences, we get $2n$ equations:
$$
A \cdot \text{vec}(P) = 0
$$

where $A \in \mathbb{R}^{2n \times 12}$.

**Step 4: SVD Solution**
The solution is the right singular vector corresponding to the smallest singular value:
$$
P = \text{reshape}(v_{12}, 3, 4)
$$

**Degenerate Cases:**
* **Coplanar points**: Points lie on a plane, rank deficiency
* **Collinear points**: Points lie on a line, severe rank deficiency
* **Single point**: Insufficient constraints

---

## 16. **Nonlinear Optimization (Bundle Adjustment)**

### 16.1 Reprojection Error

$$
\hat{x}(X) = \pi(K, R, t, d;\, X),\quad
e = \|x - \hat{x}(X)\|^2
$$

Multi-view objective:
$$
E = \sum_{i,j} \|x_{ij} - \pi(P_i, X_j)\|^2
$$

### 16.2 Optimization

* Nonlinear least squares (e.g., Levenberg‚ÄìMarquardt).
* Optimize $K, R, t,$ and distortion $(k_1,k_2,p_1,p_2,\dots)$.

### üî¨ Mathematical Formulation of Bundle Adjustment

**Cost Function:**
$$
C = \sum_{i=1}^{n} \sum_{j=1}^{m} w_{ij} \|x_{ij} - \pi(P_i, X_j)\|^2
$$

where:
* $n$ cameras with poses $P_i$
* $m$ 3D points $X_j$
* $x_{ij}$ is the projection of point $j$ in camera $i$
* $w_{ij}$ is the weight (confidence)

**Parameter Vector:**
$$
\theta = [K, R_1, t_1, \dots, R_n, t_n, X_1, \dots, X_m]
$$

**Jacobian Matrix:**
$$
J = \frac{\partial \pi}{\partial \theta}
$$

**Normal Equations:**
$$
(J^T W J) \Delta \theta = J^T W r
$$

where $r$ is the residual vector and $W$ is the weight matrix.

**Levenberg-Marquardt Update:**
$$
(J^T W J + \lambda I) \Delta \theta = J^T W r
$$

**Convergence Criteria:**
* Relative change in cost: $\frac{|C_{k+1} - C_k|}{C_k} < \epsilon$
* Gradient norm: $\|J^T r\| < \epsilon$
* Maximum iterations reached

---

## 17. **Practical Calibration Pipelines**

### 17.1 Checkerboard Calibration

1. Print checkerboard of known square size.
2. Capture many views at varied orientations.
3. Detect corners ‚Üí build 2D‚Äì3D correspondences.
4. DLT for initial $P$ ‚Üí nonlinear refinement.

### 17.2 OpenCV Implementation

* `findChessboardCorners()`, `calibrateCamera()`
* Returns $K$, distortion coeffs, and per-image $(R_i,t_i)$.

### 17.3 Evaluating Calibration Accuracy

1. **Reprojection error**: RMS < ~0.5 px (depends on resolution).
2. **Cross-validation**: hold-out images.
3. **Stability**: many images; cover full FOV.

### üî¨ Mathematical Evaluation Metrics

**Reprojection Error:**
$$
\text{RMS} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} \|x_i - \hat{x}_i\|^2}
$$

**Mean Reprojection Error:**
$$
\text{Mean Error} = \frac{1}{N} \sum_{i=1}^{N} \|x_i - \hat{x}_i\|
$$

**Standard Deviation:**
$$
\sigma = \sqrt{\frac{1}{N-1} \sum_{i=1}^{N} (\|x_i - \hat{x}_i\| - \text{Mean Error})^2}
$$

**Calibration Uncertainty:**
From the covariance matrix of the estimated parameters:
$$
\Sigma_\theta = (J^T W J)^{-1}
$$

The standard error of parameter $\theta_i$ is:
$$
\sigma_{\theta_i} = \sqrt{\Sigma_{\theta,ii}}
$$

---

# üì∑ Part VI: Advanced Topics in Camera Models

---

## 18. **Special Camera Models**

### 18.1 Fish-eye Cameras

Very wide FOV; rays mapped with nonlinear radial functions. Let $\theta$ be the angle from optical axis:

* Equidistant: $ r = f \theta $
* Equisolid: $ r = 2f \sin(\theta/2) $
* Stereographic: $ r = 2f \tan(\theta/2) $
* Orthographic: $ r = f \sin\theta $

### 18.2 Catadioptric Cameras

Lenses + curved mirrors, often with single effective viewpoint. Unified sphere model:

$$
x = \frac{X}{Z+\xi \sqrt{X^2+Y^2+Z^2}},\quad
y = \frac{Y}{Z+\xi \sqrt{X^2+Y^2+Z^2}}
$$

### 18.3 Omnidirectional Cameras

Full 360¬∞ coverage; spherical mapping:

$$
u = \arctan2(Y,X),\quad
v = \arccos\!\left(\frac{Z}{\sqrt{X^2+Y^2+Z^2}}\right)
$$

Unwrap to equirectangular for panoramic images.

### üî¨ Mathematical Analysis of Special Camera Models

**Fish-eye Projection Functions:**
The general fish-eye projection is:
$$
r = f \cdot g(\theta)
$$

where $g(\theta)$ is the projection function:

**Equidistant:**
$$
g(\theta) = \theta
$$

**Equisolid:**
$$
g(\theta) = 2 \sin(\theta/2)
$$

**Stereographic:**
$$
g(\theta) = 2 \tan(\theta/2)
$$

**Orthographic:**
$$
g(\theta) = \sin(\theta)
$$

**Mathematical Properties:**
* **Equidistant**: Preserves angular distances
* **Equisolid**: Preserves solid angles
* **Stereographic**: Conformal mapping
* **Orthographic**: Preserves area ratios

**Catadioptric Camera Model:**
The unified sphere model parameterizes all central catadioptric cameras:
$$
\xi = \frac{d}{2f}
$$

where $d$ is the mirror diameter and $f$ is the focal length.

**Omnidirectional Camera:**
Spherical coordinates $(r, \theta, \phi)$:
$$
r = \sqrt{X^2 + Y^2 + Z^2}
$$

$$
\theta = \arctan2(Y, X)
$$

$$
\phi = \arccos(Z/r)
$$

---

## 19. **Camera Model Extensions**

### 19.1 Projective Ambiguity & Self-Calibration

For any invertible $4\times4$ $H$: $P' = P H,\ X' = H^{-1} X$ gives the same images $x\sim PX = P'X'$.

Use the Image of the Absolute Conic $\omega=K^{-\top}K^{-1}$ for self-calibration constraints.

### 19.2 Multi-View Geometry Basics

$$
x'^T F x = 0,\quad E = K'^T F K
$$

Decompose $E$ to get relative pose $(R,t)$.

### 19.3 Absolute vs. Relative Camera Pose

**Relative**: pose of cam 2 w.r.t cam 1. **Absolute**: pose in world frame via PnP with known 3D‚Äì2D matches.

### üî¨ Mathematical Analysis of Projective Ambiguity

**Projective Ambiguity:**
The camera matrix $P$ and 3D points $X$ can be transformed by any invertible $4 \times 4$ matrix $H$:
$$
P' = P H, \quad X' = H^{-1} X
$$

This gives the same projections:
$$
x \sim P X \sim P' X'
$$

**Self-Calibration Constraints:**
The Image of the Absolute Conic (IAC) $\omega$ is invariant under rigid transformations:
$$
\omega = K^{-\top} K^{-1}
$$

**Kruppa Equations:**
For two views with fundamental matrix $F$:
$$
\omega = F \omega' F^{\top}
$$

**Modulus Constraint:**
For three views, the modulus constraint provides additional constraints on the IAC.

---

## 20. **Modern Applications of Camera Models**

### 20.1 Augmented Reality & Pose Estimation

Align graphics by estimating pose $(R,t)$ s.t. $ \hat{x} = K [R|t] X$ aligns with features.

### 20.2 SLAM & Visual Odometry

Minimize
$$
E = \sum_{i,j} \| x_{ij} - \pi(K, R_i, t_i, X_j)\|^2
$$
over poses and 3D structure.

### 20.3 Neural Rendering (NeRFs, Differentiable Cameras)

Each pixel casts a ray:
$$
\mathbf{r}(t) = C + t \, R \, K^{-1}(u,v,1)^T
$$
Volume rendering integrates along rays to produce color; differentiable projection is key for learning.

### üî¨ Mathematical Foundations of Modern Applications

**Pose Estimation (PnP):**
Given 3D-2D correspondences, solve:
$$
\min_{R,t} \sum_i \|x_i - \pi(K, R, t; X_i)\|^2
$$

**Direct Linear Transform (DLT) for PnP:**
For each correspondence:
$$
\begin{bmatrix} X_i^T & 0^T & -u_i X_i^T \\ 0^T & X_i^T & -v_i X_i^T \end{bmatrix}
\begin{bmatrix} r_1 \\ r_2 \\ t \end{bmatrix} = 0
$$

**Neural Radiance Fields (NeRFs):**
The volume rendering equation:
$$
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) c(\mathbf{r}(t), \mathbf{d}) dt
$$

where:
* $T(t) = \exp(-\int_{t_n}^{t} \sigma(\mathbf{r}(s)) ds)$ is the transmittance
* $\sigma(\mathbf{r}(t))$ is the volume density
* $c(\mathbf{r}(t), \mathbf{d})$ is the view-dependent color

**Differentiable Camera Models:**
The camera projection becomes differentiable:
$$
\frac{\partial \pi}{\partial \theta} = \frac{\partial \pi}{\partial K} \frac{\partial K}{\partial \theta}
$$

This enables end-to-end learning of camera parameters.

---

‚úÖ **Wrap-Up**: Special models (fisheye, catadioptric, omni) extend beyond pinhole; multi-view geometry couples cameras; calibration resolves metric scale; modern AR/SLAM/NeRFs critically rely on accurate camera models. 
  </script>
  <script>

    // Load and render tutorial content
    function loadTutorial() {
      try {
        const container = document.getElementById('tutorial-content');
        const mdNode = document.getElementById('tutorial-md');
        const tutorialContent = mdNode ? mdNode.textContent : '';
        
        // Configure marked
        marked.setOptions({
          breaks: true,
          gfm: true,
          headerIds: true
        });
        
        // Render markdown
        container.innerHTML = marked.parse(tutorialContent || '# Error\nContent missing.');
        
        // Add smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(link => {
          link.addEventListener('click', (e) => {
            e.preventDefault();
            const target = document.querySelector(link.getAttribute('href'));
            if (target) {
              target.scrollIntoView({ behavior: 'smooth' });
            }
          });
        });
        
      } catch (error) {
        document.getElementById('tutorial-content').innerHTML = `
          <div style="text-align: center; padding: 40px; color: #666;">
            <h3>Error Loading Content</h3>
            <p>Sorry, the tutorial content could not be loaded.</p>
            <p><small>${error.message}</small></p>
          </div>
        `;
      }
    }

    // Load content when page loads
    document.addEventListener('DOMContentLoaded', loadTutorial);
  </script>
</body>
</html>
